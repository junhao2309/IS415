---
title: "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods"
date: "6 March 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# Setting the Scene

Housing is an essential component of household wealth worldwide.
Buying a housing has always been a major investment for most people.
The price of housing is affected by many factors.
Some of them are global in nature such as the general economy of a country or inflation rate.
Others can be more specific to the properties themselves.
These factors can be further divided to structural and locational factors.
Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property.
Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method.
However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions.
With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998).
In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## The Task

In this take-home exercise, I am tasked to predict HDB resale prices at the sub-market level (HDB 5-room) for the month of January and February 2023 in Singapore.
The predictive models must be built by using by using conventional OLS method and GWR methods.
The data set used to train the model is from January 2021 to December 2022.

## Packages used:

**The R packages we'll use for this analysis are:**

-   [**sf**](https://cran.r-project.org/web/packages/sf/index.html): used for importing, managing, and processing geospatial data

-   [**tidyverse**](https://www.tidyverse.org/): a collection of packages for data science tasks

-   [**tmap**](https://cran.r-project.org/web/packages/tmap/index.html): used for creating thematic maps, such as choropleth and bubble maps

-   [**spdep**](https://cran.r-project.org/web/packages/spdep/index.html): used to create spatial weights matrix objects, global and local spatial autocorrelation statistics and related calculations (e.g. spatially lag attributes)

-   [**onemapsgapi**](https://cran.r-project.org/web/packages/onemapsgapi/index.html): used to query Singapore-specific spatial data, alongside additional functionalities.
    Recommended readings: [Vignette](https://cran.r-project.org/web/packages/onemapsgapi/vignettes/onemapsgapi_vignette.html) and [Documentation](https://www.onemap.gov.sg/docs/)

-   [**httr**](https://cran.r-project.org/web/packages/httr/) **: used to make API calls, such as a GET request**

-   [**units**](https://cran.r-project.org/web/packages/units/index.html): used to for manipulating numeric vectors that have physical measurement units associated with them

-   [**matrixStats**](https://cran.r-project.org/web/packages/matrixStats/index.html): a set of high-performing functions for operating on rows and columns of matrices

-   [**jsonlite**](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-aaquickstart.html): a JSON parser that can convert from JSON to the appropraite R data types

-   [**coorplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) + [**ggpubr**](https://cran.r-project.org/web/packages/ggpubr/index.html): both are used for multivariate data visualisation & analysis

-   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/index.html): provides a collection of localised spatial statistical methods, such as summary statistics, principal components analysis, discriminant analysis and various forms of GW regression

-   [**devtools**](https://cran.r-project.org/web/packages/devtools/index.html): used for installing any R packages which is not available in RCRAN.
    In this context, it'll be used to download the [**xaringanExtra**](https://pkg.garrickadenbuie.com/xaringanExtra/#/) package for [panelsets](https://pkg.garrickadenbuie.com/xaringanExtra/#/panelset)

-   [**kableExtra**](https://haozhu233.github.io/kableExtra/): an extension of kable, used for table customisation

**The following tidyverse packages will be used:**

-   **readr** for importing delimited files (.csv)

-   **tidyr** for manipulating and tidying data

-   **dplyr** for wrangling and transforming data

-   **ggplot2** for visualising data

```{r}
pacman::p_load(tidyverse, sf, sfdep, tmap, httr, jsonlite, matrixStats, readr, GWmodel, SpatialML, rsample, Metrics)
```

## Data used:

```{r}
#| code-fold: true
#| code-summary: "Show code"
# initialise a dataframe of our aspatial and geospatial dataset details
datasets <- data.frame(
  Type=c("Aspatial",
         "Geospatial",
         "Geospatial",
         "Geospatial",
         
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         "Geospatial - Onemap",
         
         "Aspatial - Self-Sourced",
         "Aspatial - Self-Sourced"),
  
  Name=c("Resale Flat Prices",
         "Master Plan 2019 Subzone Boundary (Web)",
         "MRT & LRT Exit Locations",
         "Bus Stop Locations",
         
         "Childcare Services",
         "Eldercare Services",
         "Hawker Centres",
         "Kindergartens",
         "Parks",
         "Supermarkets",
         "Community Clubs",
         "Family Services",
         "Registered Pharmacies",
         
         "Schools",
         "Shopping Mall SVY21 Coordinates`"),
  
  Format=c(".csv", 
           ".shp", 
           ".shp", 
           ".shp", 
           
           ".shp", 
           ".shp", 
           ".shp", 
           ".shp",
           ".shp", 
           ".shp",
           ".shp",
           ".shp",
           ".shp",
           
           ".csv",
           ".csv"),
  
  Source=c("[data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)",
           "data.gov.sg",
           "[LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=Train)",
           "[LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop)",
           
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           
           "[data.gov.sg](https://data.gov.sg/dataset/school-directory-and-information)",
           "[Mall SVY21 Coordinates Web Scaper](https://github.com/ValaryLim/Mall-Coordinates-Web-Scraper)")
  )

# with reference to this guide on kableExtra:
# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html
# kable_material is the name of the kable theme
# 'hover' for to highlight row when hovering, 'scale_down' to adjust table to fit page width
library(knitr)
library(kableExtra)
kable(datasets, caption="Datasets Used") %>%
  kable_material("hover", latex_options="scale_down")

```

### How to extract data from onemap

These are the steps:

*The code chunks below uses onemapsgapi package.*

1.  Register an account with [onemap](https://developers.onemap.sg/register/)
2.  A code is then be sent to your email. Then fill in this [form](https://developers.onemap.sg/confirm_account/)
3.  In the console, run the code chunk below:

```{r, eval = FALSE}
token <- get_token("email", "password")
```

Input the email and password that you've registered with onemap.
This will provide you a token ID under objectname: token.
Note that you will need to do this again as the token is only valid for 3 days.

4.  Obtain queryname by running the code chunk below:

```{r, eval = FALSE}
themes <- search_themes(token)
```

Input your token ID and you can source for the queryname for Step 5.

5.  Use get_theme() to get the data from onemap

For this example, we will use queryname, "eldercare".

```{r, eval = FALSE}
eldercare<-get_theme(token,"eldercare")
```

6.  Convert the object into an sf object then download it into your data folder. st_as_sf() is for converting the file and st_write() is for writing the file into your folder. st_transform() sets the coordinate reference system to Singapore, EPSG::3414. These functions are from the sf package.

```{r, eval = FALSE}
eldercaresf <- st_as_sf(eldercare, coords=c("Lng", "Lat"), 
                        crs=4326) %>% 
  st_transform(crs = 3414)
st_write(obj = eldercaresf,
         dsn = "data/geospatial",
         layer = "eldercare",
         driver = "ESRI Shapefile")
```

To make it more automatic, define which variables you want from the onemap database into a vector.
The code chunk runs a for loop that does steps 5 and 6 together and stores them into your folder.

```{r, eval = FALSE}
onemap_variables <- c("childcare", "communityclubs", "eldercare", "family", "hawkercentre", "kindergartens", "nationalparks","registered_pharmacy","supermarkets")

df <- list()
df_sf <- list()
for (i in 1:length(onemap_variables)){
  df[[i]] <- get_theme(token, onemap_variables[i])
  df_sf[[i]] <- st_as_sf(df[[i]], coords=c("Lng", "Lat"), 
                        crs=4326) %>%
    st_transform(crs = 3414)
st_write(obj = df_sf[[i]],
         dsn = "data/geospatial/Onemap",
         layer = onemap_variables[i],
         driver = "ESRI Shapefile")
}
```

# Load Data into R

## Geospatial Data

::: panel-tabset
### One-Map

We will make use of st_read() from the sf package to load the data in

```{r}
pharmacy <- st_read(dsn = "data/geospatial/Onemap",
                layer = "registered_pharmacy")
parks <- st_read(dsn = "data/geospatial/Onemap",
                 layer = "nationalparks")
kindergartens <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "kindergartens")
hawker <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "hawkercentre")
eldercare <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "eldercare")
communityclubs <-st_read(dsn = "data/geospatial/Onemap",
                    layer = "communityclubs")
supermarkets <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "SUPERMARKETS")
familyservices <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "family")
childcare <- st_read(dsn = "data/geospatial/Onemap",
                     layer = "childcare")
```

### Geospatial Map

We will make use of st_read() from the sf package to load the data in

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                layer = "MP14_SUBZONE_WEB_PL")
```

### DATA.GOV

We will make use of st_read() from the sf package to load the data in

```{r}
Bus_stop <- st_read(dsn = "data/geospatial",
                layer = "BusStop")
MRT <- st_read(dsn = "data/geospatial/lta-mrt-station-exit-kml.kml")
```

### Aspatial Data

We will make use of read_csv() from the readr package to read the csv file into Rstudio

```{r}
Resale <- read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
```

```{r}
Schools <- read_csv("data/aspatial/schools.csv")
```

```{r}
Malls <- read_csv("data/aspatial/mall_coordinates_updated.csv")
```
:::

# Data Wrangling

## HDB Resale Price

For the purpose of this take-home exercise, we will only be using five-room flat and training transaction period should be from January 2021 and December 2022.

From the output above, the variables we want are:

-   Resale Price
-   Month
-   Flat Type
-   Area of the unit
-   Floor level
-   Remaining Lease
-   Flat Model

### Story and Date adjustents

We will see the unique values of story by running the code chunk below.

```{r}
unique(Resale$storey_range)
```

As we can see, there are 17 factor levels.

When dealing with categorical variables, there are several ways to manipulate the data.

These are some ways to deal with categorical variables:

1.  Set it as a factor, to which R will recognize them as binary variables automatically.
2.  Set categorical variable as multiple binary variable columns, assigning values 1 and 0.
3.  Set categorical variables as ordinal numbers.

In the case of story levels, as they are ascending in nature, we can use No. 3, which is to set them as ordinal variables.

The code chunk below will sequence the story levels, assigning value 1 to "01 TO 03", 2 to "04 TO 06", ..., 17 to "49 TO 51".

```{r, eval = FALSE}
# Define the story levels and ordinal values
story_levels <- c("01 TO 03", "04 TO 06", "07 TO 09", "10 TO 12", "13 TO 15", "16 TO 18", "19 TO 21", "22 TO 24", "25 TO 27", "28 TO 30", "31 TO 33", "34 TO 36", "37 TO 39", "40 TO 42", "43 TO 45", "46 TO 48", "49 TO 51")
story_ordinal <- seq_along(story_levels)

# Create the ordinal variable based on the story column
Resale$Story_Ordinal <- story_ordinal[match(Resale$storey_range, story_levels)]

# Set the labels for the ordinal variable
levels(Resale$Story_Ordinal) <- story_levels
```

### Date

Next we will set the Date column as date type and also ensure Story_Ordinal is of numeric type.

To check the data set, you can run glimpse() from the dplyr package to see the data types of each variable.

```{r, eval = FALSE}
Resale <- Resale %>%
  mutate(month = as.Date(paste0(month, "-01"), format ="%Y-%m-%d"),
         Story_Ordinal = as.numeric(Story_Ordinal))
```

### Flat Model

Next, we will set Flat Model as a binary variable by running the code chunk below.

```{r, eval = FALSE}
Resale <- Resale %>%
  tidyr::pivot_wider(names_from = flat_model,
              values_from = flat_model, 
              values_fn = list(flat_model = ~1), 
              values_fill = 0)
```

### Remaining Lease

We will settle the remaining lease by running the code chunk below: This turns remaining_lease from chr type to numeric in terms of units, years.

```{r, eval = FALSE}
# Splits the string by year and month, using str_split from the stringr package
str_list <- str_split(Resale$remaining_lease, " ")

for (i in 1:length(str_list)) {
  if (length(unlist(str_list[i])) > 2) {
      year <- as.numeric(unlist(str_list[i])[1])
      month <- as.numeric(unlist(str_list[i])[3])
      Resale$remaining_lease[i] <- year + round(month/12, 2)
  }
  else {
    year <- as.numeric(unlist(str_list[i])[1])
    Resale$remaining_lease[i] <- year
  }
}

Resale <- Resale %>%
  mutate(remaining_lease =as.numeric(remaining_lease))
```

We can take a look at the dataframe now by using glimpse() from the dplyr package

```{r}
glimpse(Resale)
```

We will first filter out the relevant columns we want by running the code chunk below:

select() helps to select the columns we want and filter() helps us to filter to the specific two months.
These two functions come from the dplyr package.

::: panel-tabset
### Training Data

```{r, eval = FALSE}
Resale_train <- Resale %>%
  filter(month >= "2021-01-01" & month <= "2022-12-01",
         flat_type == "5 ROOM") %>%
  dplyr::select(-2,-3,-6,-8)
```

We will save Resale_train as a rds file for easy retrieval

```{r, eval = FALSE}
write_rds(Resale_train, "data/aspatial/Resale_train.rds")
```

```{r}
Resale_train <- read_rds("data/aspatial/Resale_train.rds")
```

```{r}
glimpse(Resale_train)
```

### Test Data

```{r, eval = FALSE}
Resale_test <- Resale %>%
  filter(month >="2023-01-01" & month <="2023-02-01",
         flat_type == "5 ROOM") %>%
  dplyr::select(-2,-3,-6,-8)
```

We will save Resale_train as a rds file for easy retrieval

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(Resale_test, "data/aspatial/Resale_test.rds")
```

```{r}
Resale_test <- read_rds("data/aspatial/Resale_test.rds")
```

```{r}
glimpse(Resale_test)
```
:::

You may be wondering why I had removed LeaseBegin as that denotes the age of the apartment.
This is due to the issue of high multicollinearity in statistics.
Intuitively, HDB only has 99year lease and as such, remaining_lease has a perfect inverse relationship to LeaseBegin.
For example a 9 year old apartment would have a remaining lease of (99-9) = 90 years.

### Inserting geometries

Next, once our data set has been cleaned to its relevant variables, we will insert the geometries of the resale apartment location.

To do this, we will have to obtain the geometries from this [url](https://developers.onemap.sg/commonapi/search).

This code is referenced from [Megan's work](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/).

```{r, eval = FALSE}
library("httr")
geocode <- function(block, streetname) {
  base_url <- "https://developers.onemap.sg/commonapi/search"
  address <- paste(block, streetname, sep = " ")
  query <- list("searchVal" = address, 
                "returnGeom" = "Y",
                "getAddrDetails" = "N",
                "pageNum" = "1")
  
  res <- GET(base_url, query = query)
  restext<-content(res, as="text")
  
  output <- jsonlite::fromJSON(restext)  %>% 
    as.data.frame() %>%
    dplyr::select("results.LATITUDE", "results.LONGITUDE")
  return(output)
}
```

In both Training and Test Data, we will use this function and a for loop that iterates between each row of the dataframe.

This is done to input the block and street number into the function, geocode() and inputs the results into the LATITUDE and LONGITUDE columns.

::: panel-tabset
#### Training Data

```{r, eval = FALSE}
Resale_train$LATITUDE <- 0
Resale_train$LONGITUDE <- 0

for (i in 1:nrow(Resale_train)){
  temp_output <- geocode(Resale_train[i, 2], Resale_train[i, 3])
  
  Resale_train$LATITUDE[i] <- temp_output$results.LATITUDE
  Resale_train$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

#### Testing Data

```{r, eval = FALSE}
Resale_test$LATITUDE <- 0
Resale_test$LONGITUDE <- 0

for (i in 1:nrow(Resale_test)){
  temp_output <- geocode(Resale_test[i, 2], Resale_test[i, 3])
  
  Resale_test$LATITUDE[i] <- temp_output$results.LATITUDE
  Resale_test$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```
:::

### Convert into Resale_2023 dataframe into sf

By using st_as_sf() from the sf package, we can convert Resale_2023 into an sf and then transform the crs to EPSG:: 3414 which is the coordinate reference system for Singapore.

::: panel-tabset
#### Training Data

```{r, eval = FALSE}
Resale_training_sf <- st_as_sf(Resale_train, 
                      coords = c("LONGITUDE", "LATITUDE"),
                      crs = 4326) %>%
  st_transform(crs = 3414)
```

We can store it as a shapefile for future retrieval.

```{r,eval = FALSE}
st_write(Resale_training_sf, 
         dsn = "data/aspatial", 
         layer = "resale_train_sf",
         driver = "ESRI Shapefile")
```

```{r}
Resale_training_sf <- st_read(dsn = "data/aspatial",
                     layer = "resale_train_sf")
```

#### Test Data

```{r, eval = FALSE}
Resale_test_sf <- st_as_sf(Resale_test, 
                      coords = c("LONGITUDE", "LATITUDE"),
                      crs = 4326) %>%
  st_transform(crs = 3414)
```

We can store it as a shapefile for future retrieval

```{r,eval = FALSE}
st_write(Resale_test_sf, 
         dsn = "data/aspatial", 
         layer = "Resale_test_sf",
         driver = "ESRI Shapefile")
```

```{r}
Resale_test_sf <- st_read(dsn = "data/aspatial",
                     layer = "Resale_test_sf")
```
:::

## Aspatial Data Wrangling

For schools, the dataframe holds several levels of schools.
For the purpose the regression, we will focus only on primary and secondary schools.

```{r}
unique(Schools$mainlevel_code)
```

We will create the respective primary school and secondary school dataframe.

We will also make some tweaks to the geocode function created above, that takes in postal code instead of block and street.

```{r}
geocode <- function(postal) {
  base_url <- "https://developers.onemap.sg/commonapi/search"
  query <- list("searchVal" = postal, 
                "returnGeom" = "Y",
                "getAddrDetails" = "N",
                "pageNum" = "1")
  
  res <- GET(base_url, query = query)
  restext<-content(res, as="text")
  
  output <- jsonlite::fromJSON(restext)  %>% 
    as.data.frame() %>%
    dplyr::select("results.LATITUDE", "results.LONGITUDE")
  return(output)
}
```

::: panel-tabset
### Primary School

```{r}
Primary <- Schools %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name,postal_code)
```

```{r, eval = FALSE}
Primary$LATITUDE <- 0
Primary$LONGITUDE <- 0

for (i in 1:nrow(Primary)){
  temp_output <- geocode(Primary[i, 2])
  
  Primary$LATITUDE[i] <- temp_output$results.LATITUDE
  Primary$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

```{r, eval =FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(Primary, "data/aspatial/Primary.rds")
```

```{r}
Primary <- read_rds("data/aspatial/Primary.rds")
```

### Secondary School

```{r, eval = FALSE}
Secondary <- Schools %>%
  filter(mainlevel_code == "SECONDARY") %>%
  select(school_name,postal_code)
```

Next, we will change the postal code of [ZHENGHUA SECONDARY SCHOOL](https://www.moe.gov.sg/schoolfinder/schooldetail?schoolname=zhenghua-secondary-school) as it has the wrong postal code.
We found out about this after checking that there was an error after running the geocode.
Hence, it is a good habit to check on your dataframe for any missing geometries before moving forward.

```{r, eval = FALSE}
Secondary[137,2]<- "679962"
```

We can rerun the code.

```{r, eval = FALSE}
Secondary$LATITUDE <- 0
Secondary$LONGITUDE <- 0

for (i in 1:nrow(Secondary)){
  temp_output <- geocode(Secondary[i, 2])
  
  Secondary$LATITUDE[i] <- temp_output$results.LATITUDE
  Secondary$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(Secondary, file = "data/aspatial/Secondary.rds")
```

```{r}
Secondary <- read_rds("data/aspatial/Secondary.rds")
```
:::

For Primary_sch, Secondary_sch and Mall, we need to convert them into sf and transform it to the correct CRS.

Primary School:

```{r}
Primary_sf <- Primary %>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"),
           crs = 4326) %>% 
  st_transform(crs = 3414)
```

Good Primary Schools:

We will refer to [schlah](https://schlah.com/primary-schools) list of good primary schools.
Note that CANOSSA HIGH SCHOOL in schlah webpage is known as CANOSSA CATHOLIC PRIMARY SCHOOL.

```{r}
Good_Prisch <- Primary_sf %>%
  filter(school_name %in% c("NANYANG PRIMARY SCHOOL",
                            "TAO NAN SCHOOL",
                            "CANOSSA CATHOLIC PRIMARY SCHOOL",
                            "NAN HUA PRIMARY SCHOOL",
                            "ST. HILDA'S PRIMARY SCHOOL",
                            "HENRY PARK PRIMARY SCHOOL",
                            "ANGLO-CHINESE SCHOOL (PRIMARY)",
                            "RAFFLES GIRLS' PRIMARY SCHOOL",
                            "PEI HWA PRESBYTERIAN PRIMARY SCHOOL"
                            ))
```

Secondary School:

```{r}
Secondary_sf <- Secondary %>%
  st_as_sf(coords = c("LONGITUDE","LATITUDE"),
           crs = 4326) %>% 
  st_transform(crs = 3414)
```

Malls:

```{r}
malls <- Malls %>%
  st_as_sf(coords = c("longitude", "latitude"),
           crs = 4326) %>%
  st_transform(crs= 3414)
```

CBD Area:

As for the CBD area, we will use a centre point coordinate to illustrate proximity to CBD Area.

```{r}
lat <- 1.287953
lng <- 103.851784

cbd_sf <- data.frame(lat, lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs=4326) %>%
  st_transform(crs=3414)
```

## Ensure all datasets are in EPSG:3414

Aside from the onemap variables, we need to check the geospatial data and the other data sets, primary school, secondary school and shopping malls.

```{r}
st_crs(mpsz) ; st_crs(Bus_stop) ; st_crs(MRT) ; st_crs(supermarkets)
```

We notice that the CRS for these 3 geospatial data are not correct.

We will use st_transformed as taught previously to convert it to EPSG:3414.

```{r}
mpsz <- mpsz %>%
  st_transform(crs = 3414)
```

```{r}
Bus_stop <- Bus_stop %>%
  st_transform(crs = 3414)
```

```{r}
MRT <- MRT %>%
  st_transform(crs = 3414)
```

```{r}
supermarkets <- supermarkets %>%
  st_transform(crs = 3414)
```

## Checking invalid geometries

We should also check for any invalid geometries in our data.

::: panel-tabset
### Onemap variable checks

```{r}
length(which(st_is_valid(communityclubs)== FALSE))
```

```{r}
length(which(st_is_valid(eldercare)== FALSE))
```

```{r}
length(which(st_is_valid(familyservices)== FALSE))
```

```{r}
length(which(st_is_valid(hawker)== FALSE))
```

```{r}
length(which(st_is_valid(kindergartens)== FALSE))
```

```{r}
length(which(st_is_valid(parks)== FALSE))
```

```{r}
length(which(st_is_valid(pharmacy)== FALSE))
```

```{r}
length(which(st_is_valid(supermarkets)== FALSE))
```

```{r}
length(which(st_is_valid(childcare)== FALSE))
```

We can see that for the oneMap variables, there are no invalid geometries.

### DATA.GOV

```{r}
length(which(st_is_valid(mpsz)== FALSE))
```

```{r}
length(which(st_is_valid(Bus_stop)== FALSE))
```

```{r}
length(which(st_is_valid(MRT)== FALSE))
```

For the data obtained from DATA.GOV, there are some invalid geometries

We will proceed to remove them

```{r}
mpsz <- st_make_valid(mpsz)
```

### Aspatial Data

```{r}
length(which(st_is_valid(Primary_sf)== FALSE))
```

```{r}
length(which(st_is_valid(Secondary_sf)== FALSE))
```

```{r}
length(which(st_is_valid(malls)==FALSE))
```

There is no invalid geometries in the aspatial data set.
:::

## Removing Unnecessary columns

We will only need the name and the geometries so we can exclude the rest of the columns

::: panel-tabset
### Onemap

```{r}
communityclubs <- communityclubs %>%
  select(1)
```

```{r}
eldercare <- eldercare %>%
  select(1)
```

```{r}
familyservices <- familyservices %>%
  select(1)
```

```{r}
hawker <- hawker %>%
  select(1)
```

```{r}
kindergartens <- kindergartens %>%
  select(1)
```

```{r}
parks <- parks %>%
  select(1)
```

```{r}
pharmacy <- pharmacy %>%
  select(1)
```

```{r}
supermarkets <- supermarkets %>%
  select(1)
```

```{r}
childcare <- childcare %>%
  select(1)
```

### DATA.GOV

```{r}
Bus_stop <- select(Bus_stop, 1)
```

For MRT, we will need to drop the Z-dimension.
This can be seen when you View(MRT).

```{r}
MRT_Station <- st_zm(MRT) %>%
  select(1)
```

### Aspatial Data

```{r}
Primary_sf <- select(Primary_sf, 1)
```

```{r}
Secondary_sf <- select(Secondary_sf, 1)
```

```{r}
malls <- select(malls, name)
```
:::

## Check NA values

::: panel-tabset
### Onemap

```{r}
communityclubs[rowSums(is.na(communityclubs))!=0,]
```

```{r}
eldercare[rowSums(is.na(eldercare))!=0,]
```

```{r}
familyservices[rowSums(is.na(familyservices))!=0,]
```

```{r}
hawker[rowSums(is.na(hawker))!=0,]
```

```{r}
kindergartens[rowSums(is.na(kindergartens))!=0,]
```

```{r}
parks[rowSums(is.na(parks))!=0,]
```

```{r}
pharmacy[rowSums(is.na(pharmacy))!=0,]
```

```{r}
supermarkets[rowSums(is.na(supermarkets))!=0,]
```

```{r}
childcare[rowSums(is.na(childcare))!=0,]
```

We notice that family services has NA values, we will proceed to remove them using na.omit()

```{r}
familyservices<- na.omit(familyservices, c("ADDRESSBUI"))
```

### DATA.GOV

```{r}
Bus_stop[rowSums(is.na(Bus_stop))!=0,]
```

```{r}
MRT_Station[rowSums(is.na(MRT_Station))!=0,]
```

```{r}
mpsz[rowSums(is.na(mpsz))!=0,]
```

### Aspatial Data

```{r}
length(which(is.na(Primary_sf) == TRUE))
```

```{r}
length(which(is.na(Secondary_sf) == TRUE))
```

```{r}
length(which(is.na(malls) == TRUE))
```

There is no NA values in the aspatial data
:::

# Visualisation

We will visualize each variable on its own to see how they are being scattered.

This is done by using tm_shape(), tm_polygons, tm_dots() from the tmap package.
Since these maps are set to "plot", mpsz sets the Singapore boundaries while tm_dots plots the points onto the polygon.

::: panel-tabset
### Subzone

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons()
```

### MRT Stations

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(MRT_Station) +
  tm_dots()
```

### Bus Stops

```{r}
tmap_mode("plot")
tm_shape(mpsz) + 
  tm_polygons() +
tm_shape(Bus_stop) +
  tm_dots(col = "red",
          size = 0.0075)
```

Note that the points outside of Singapore boundaries are related to bus stop at Johor Bahru and they are valid points to our analysis.

### Community Club

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(communityclubs) +
  tm_dots()
```

### Eldercare

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(eldercare) +
  tm_dots()
```

### Family Service

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(familyservices) +
  tm_dots()
```

### Hawker Centre

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hawker) +
  tm_dots()
```

### Kindergarten

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(kindergartens) +
  tm_dots()
```

### Malls

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(malls) +
  tm_dots()
```

### Parks

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(parks) +
  tm_dots()
```

### Pharmacies

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(pharmacy) +
  tm_dots()
```

### Childcare

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(childcare) +
  tm_dots()
```

### Primary School

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(Primary_sf) +
  tm_dots()
```

### Secondary School

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(Secondary_sf) +
  tm_dots()
```
:::

# Proximity distance calculator

Here, we will need to calculate the distance between each variable to the resale apartment.

The code below creates a function called proximity() that takes in 3 values.

```{r, eval = FALSE}
library(units)
library(matrixStats)
proximity <- function(df1, df2, varname) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units()
  df1[,varname] <- rowMins(dist_matrix)
  return(df1) }
```

The code chunks in training and test data essentially calculates the proximity between Resale flats and the variable.
This is piped each time, creating a new column with the 3rd input name and assigned to the object, training_resale/test_resale.

::: panel-tabset
## Training Data

```{r, eval = FALSE}
training_resale <- 
  # the columns will be truncated later on when viewing 
  # so we're limiting ourselves to two-character columns for ease of viewing between
  proximity(Resale_training_sf, cbd_sf, "PROX_CBD") %>%
  proximity(.,Bus_stop, "PROX_BUS") %>%
  proximity(., communityclubs, "PROX_CLUBS") %>%
  proximity(., eldercare, "PROX_ELDERCARE") %>%
  proximity(., familyservices, "PROX_FAM") %>%
  proximity(., MRT_Station, "PROX_MRT") %>%
  proximity(., hawker, "PROX_HAWKER") %>%
  proximity(., kindergartens, "PROX_KINDERGARTENS") %>%
  proximity(., pharmacy, "PROX_PHARMACY") %>%
  proximity(., parks, "PROX_PARK") %>%
  proximity(., malls, "PROX_MALL") %>%
  proximity(., supermarkets, "PROX_SPRMKT") %>%
  proximity(., childcare, "PROX_CHILDCARE") %>%
  proximity(., Primary_sf, "PROX_PRISCH") %>%
  proximity(., Good_Prisch, "PROX_GOODP") %>%
  proximity(., Secondary_sf, "PROX_SECSCH") 
```

We will change the names of the column for easy recognition and referencing.
This is done using mutate() and rename() from the dplyr package

```{r, eval =FALSE}
training_resale <- training_resale %>%
  mutate() %>%
  rename("AREA_SQM" = "flr_r_s",
         "PRICE" = "rsl_prc",
         "REMAINING_LEASE" = "rmnng_l") %>%
  relocate(`PRICE`)
```

## Test Data

```{r, eval=FALSE}
test_resale <- 
  # the columns will be truncated later on when viewing 
  # so we're limiting ourselves to two-character columns for ease of viewing between
  proximity(Resale_test_sf, cbd_sf, "PROX_CBD") %>%
  proximity(.,Bus_stop, "PROX_BUS") %>%
  proximity(., communityclubs, "PROX_CLUBS") %>%
  proximity(., eldercare, "PROX_ELDERCARE") %>%
  proximity(., familyservices, "PROX_FAM") %>%
  proximity(., MRT_Station, "PROX_MRT") %>%
  proximity(., hawker, "PROX_HAWKER") %>%
  proximity(., kindergartens, "PROX_KINDERGARTENS") %>%
  proximity(., pharmacy, "PROX_PHARMACY") %>%
  proximity(., parks, "PROX_PARK") %>%
  proximity(., malls, "PROX_MALL") %>%
  proximity(., supermarkets, "PROX_SPRMKT") %>%
  proximity(., childcare, "PROX_CHILDCARE") %>%
  proximity(., Primary_sf, "PROX_PRISCH") %>%
  proximity(., Good_Prisch, "PROX_GOODP") %>%
  proximity(., Secondary_sf, "PROX_SECSCH") 
```

We will change the names of the columns for easy recognition and referencing.
This is done using mutate() and rename() from the dplyr package

```{r, eval = FALSE}
test_resale <- test_resale %>%
  mutate() %>%
  rename("AREA_SQM" = "flr_r_s",
         "PRICE" = "rsl_prc",
         "REMAINING_LEASE" = "rmnng_l") %>%
  relocate(`PRICE`)
```
:::

## Facility Count within Radius Calculation

Here, we want to find the number of facilities within a particular radius.
Like above, we'll use st_distance() to compute the distance between the flats and the desired facilities, and then sum up the observations with rowSums().
The values will be appended to the data frame as a new column.

```{r, eval = FALSE}
num_radius <- function(df1, df2, varname, radius) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units() %>%
    as.data.frame()
  df1[,varname] <- rowSums(dist_matrix <= radius)
  return(df1)
}
```

::: panel-tabset
### Training Data

```{r, eval = FALSE}
training_resale <- 
  num_radius(training_resale, kindergartens, "NUM_KNDRGTN", 350) %>%
  num_radius(., childcare, "NUM_CHILDCARE", 350) %>%
  num_radius(., Bus_stop, "NUM_BUS_STOP", 350) %>%
  num_radius(., Primary_sf, "NUM_PRISCH", 1000) %>%
  num_radius(., Secondary_sf, "NUM_SECSCH", 1000)
```

Always make sure to save the object as an rds file for future referencing.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(training_resale, "data/aspatial/training_resale.rds")
```

### Test Data

```{r, eval = FALSE}
test_resale <- 
  num_radius(test_resale, kindergartens, "NUM_KNDRGTN", 350) %>%
  num_radius(., childcare, "NUM_CHILDCARE", 350) %>%
  num_radius(., Bus_stop, "NUM_BUS_STOP", 350) %>%
  num_radius(., Primary_sf, "NUM_PRISCH", 1000) %>%
  num_radius(., Secondary_sf, "NUM_SECSCH", 1000)
```

Always make sure to save the object as an rds file for future referencing.

```{r, eval = FALSE}
write_rds(test_resale, "data/aspatial/test_resale.rds")
```
:::

# Read Data in

```{r}
training_data <- read_rds("data/aspatial/training_resale.rds")
test_data <- read_rds("data/aspatial/test_resale.rds")
```

::: panel-tabset
### Training Data

```{r}
glimpse(training_data)
```

We can see that we have forgotten to remove Block and street.
We can also check and confirm that all variables are in the right format.

```{r}
training_data <- training_data %>%
  select(-block, -strt_nm)
```

### Test Data

```{r}
glimpse(test_data)
```

We can see that we have forgotten to remove Block and street.
We can also check and confirm that all variables are in the right format.

```{r}
test_data <- test_data %>%
  select(-block, -strt_nm)
```
:::

# Computing Correlation Matrix

```{r}
training_data_nogeo <- training_data %>%
  st_drop_geometry()
```

We will first create the correlation matrix and check for any NA or infinite values.

```{r}
cor_matrix <- cor(training_data_nogeo[,3:47])
any(is.na(cor_matrix)) # check for missing values
any(is.infinite(cor_matrix)) # check for infinite values
```

Since there are missing values, we will fix this by assigning 0 to them

```{r}
na_value <- is.na(cor_matrix)
cor_matrix[na_value] <- 0
```

```{r}
corrplot::corrplot(cor_matrix, 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.2, 
                   method = "number", 
                   type = "upper")
```

We can see several correlated variables above 0.5 but they are within acceptable range to be included in the regression.
However, it would be better to remove PROX_KINDERGARTENS, PROX_CHILDCARE, PROX_BUS, PROX_PRISCH and PROX_SECSCH.
This is because the effects from these 5 variables would have been captured in the facility count, NUM\_ .
While the correlation matrix suggest no high correlation, it could create bias in our estimator should the variables be correlated.

# Building a non-spatial multiple linear regression

We will run the multiple linear regression using the base R function, lm().

```{r, eval = FALSE}
price_mlr1 <- lm(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = training_data)
```

We can first save the model by running the code chunk below.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(price_mlr1, "data/model/price_mlr1.rds")
```

```{r}
price_mlr1 <- read_rds("data/model/price_mlr1.rds")
```

## Understanding the non-spatial multiple linear regression model

```{r}
summary(price_mlr1)
```

We can visualise the regression as a table by using gtsummary package.

```{r}
gtsummary::tbl_regression(price_mlr1)
```

From the multiple linear regression model above, we notice that adjusted R square is at 0.7266.
This suggest that the model can explain 0.7266 of the variation in Resale Price.
We also notice that majority of the proximity variables are significant at the 1% level.
This can also be said for AREASQM, REMAINING LEASE AND STORY OR. The model of the apartment did not have significant impact on the Resale Price.
However, two model type, DBSS and Type.S2 are significant.

## Calculating predictive error

```{r}
lm_predicted_value <- predict.lm(price_mlr1, newdata = test_data)

# Calculate MSE
MSE <- mean((test_data$PRICE - lm_predicted_value)^2)

rmse_lm <- sqrt(MSE)
rmse_lm
```

From the square-root of the mean square error is 87822.93.
We will take note of this value in the later parts of the regression modelling.

We can compare models using Akaike information criterion (AIC).
The smaller the AIC, the better the model.
We will find the AIC by running the code chunk below.

```{r}
AIC(price_mlr1)
```

We will just take note of this number and move along with the other models.

# GWR Predictive Method

## Converting sf dataframe to SpatialPointDataframe

First, we will need to change the training dataframe into a Spatial point data frame.
This is used to calculate the other sections in this portion of the Exercise.

```{r}
train_data_sp <- as_Spatial(training_data)
train_data_sp
```

## Finding optimal adaptive bandwidth

We will find the optimal adaptive bandwidth of the geospatial weighted regression.
The code chunk below uses bw.gwr() from the GWModel package:

```{r, eval = FALSE}
bw_adaptive <- bw.gwr(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

![](images/image-1232532573.png)

The smallest CV score is adaptive bandwidth: 1005.

We will save the adaptive bandwidth as an RDS file for easy retrieval.

```{r, eval = FALSE}
write_rds(bw_adaptive, file = "data/model/bw_adaptive.rds")
```

## Constructing the adaptive bandwidth GWR

We will retrieve the adaptive bandwidth by using read_rds from the readr package.

```{r}
bw_adaptive <- read_rds("data/model/bw_adaptive.rds")
bw_adaptive
```

```{r, eval = FALSE}
gwr_adaptive <- gwr.basic(formula = PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
```

Again we will save the the regression model as an rds file.

```{r, eval = FALSE}
write_rds(gwr_adaptive, "data/model/gwr_adaptive.rds")
```

```{r}
gwr_adaptive <- read_rds("data/model/gwr_adaptive.rds")
gwr_adaptive
```

From the output above, we see that Adjusted R-square is 0.8352, higher than the non-spatial mulitiple linear regression model.

## Preparing coordinates data

### Extracting coordinates data

The code chunk below extract the x, y coordinates of the full, training and test data sets.
This is as taught in in-class exercise 9.

```{r, eval = FALSE}
coords_train <- st_coordinates(training_data)
coords_test <- st_coordinates(test_data)
```

Before we proceed, we should save the output into rds for future uses

```{r, eval = FALSE}
write_rds(coords_train, "data/model/coords_train.rds" )
write_rds(coords_test, "data/model/coords_test.rds" )
```

Afterwhich, we can just retrieve the coordinates from the saved files.

```{r}
coords_train <- read_rds("data/model/coords_train.rds")
coords_test <- read_rds("data/model/coords_test.rds")
```

### Dropping geometry field

We need to drop the geometry column of the sf data.frame by using st_drop_geometry() of the sf package.
This is because, in the later parts, we cannot have the geometry columns in the data when running the random forest model.

```{r, eval = FALSE}
train_data_nogeo <- training_data %>%
  st_drop_geometry()
```

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(train_data_nogeo, "data/model/train_data_nogeo.rds")
```

```{r}
train_data_nogeo <- read_rds("data/model/train_data_nogeo.rds")
```

## Calibrating Random Forest Model

The code chunk below uses ranger() from the ranger package

This is done to calibrate the model, for predicting HDB resale prices.

```{r}
set.seed(1234)
rf <- ranger(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_nogeo)
```

```{r}
print(rf)
```

```{r, eval = FALSE}
gwRF_bw <- grf.bw(formula = PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_nogeo,
                kernel = "adaptive",
                trees = 30,
                coords = coords_train
                )
```

Due to long computational time, we will make do with what has been generated.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
Bandwidth: 725
R2 of Local Model: 0.830196864620965
Bandwidth: 726
R2 of Local Model: 0.831511354240713
Bandwidth: 727
R2 of Local Model: 0.825814193350772
Bandwidth: 728
R2 of Local Model: 0.833117627936025
Bandwidth: 729
R2 of Local Model: 0.831542049422668
Bandwidth: 730
R2 of Local Model: 0.832690759328841
Bandwidth: 731
R2 of Local Model: 0.831409189031434
Bandwidth: 732
R2 of Local Model: 0.833478593806476
Bandwidth: 733
R2 of Local Model: 0.828881217358686
Bandwidth: 734
R2 of Local Model: 0.833837568289313
Bandwidth: 735
R2 of Local Model: 0.830944546212957
Bandwidth: 736
R2 of Local Model: 0.829109016792875
Bandwidth: 737
R2 of Local Model: 0.832693746938043
Bandwidth: 738
R2 of Local Model: 0.830609753683569
Bandwidth: 739
R2 of Local Model: 0.828506606657883
Bandwidth: 740
R2 of Local Model: 0.829942324327648
Bandwidth: 741
R2 of Local Model: 0.829673050758191
Bandwidth: 742
R2 of Local Model: 0.834731888959398
Bandwidth: 743
R2 of Local Model: 0.831271390847821
Bandwidth: 744
R2 of Local Model: 0.830326788690227
Bandwidth: 745
R2 of Local Model: 0.832112671503318
Bandwidth: 746
R2 of Local Model: 0.832734247748836
Bandwidth: 747
R2 of Local Model: 0.831818201150335
Bandwidth: 748
R2 of Local Model: 0.831135316402252
Bandwidth: 749
R2 of Local Model: 0.832132102303099
Bandwidth: 750
R2 of Local Model: 0.83134136735134
Bandwidth: 751
R2 of Local Model: 0.8306802097722
Bandwidth: 752
R2 of Local Model: 0.829479547542639
Bandwidth: 753
R2 of Local Model: 0.833479383819347
Bandwidth: 754
R2 of Local Model: 0.83099893981291
Bandwidth: 755
R2 of Local Model: 0.830071154250661
Bandwidth: 756
R2 of Local Model: 0.830529200273976
Bandwidth: 757
R2 of Local Model: 0.831649054301567
Bandwidth: 758
R2 of Local Model: 0.831735899098526
Bandwidth: 759
R2 of Local Model: 0.830897778940034
Bandwidth: 760
R2 of Local Model: 0.831347886646916
Bandwidth: 761
R2 of Local Model: 0.828948414511987
Bandwidth: 762
R2 of Local Model: 0.831341467830645
Bandwidth: 763
R2 of Local Model: 0.830232601893768
Bandwidth: 764
R2 of Local Model: 0.831059678585006
Bandwidth: 765
R2 of Local Model: 0.828842780381122
Bandwidth: 766
R2 of Local Model: 0.833282210197786
Bandwidth: 767
R2 of Local Model: 0.82944064674115
Bandwidth: 768
R2 of Local Model: 0.832924358100198
Bandwidth: 769
R2 of Local Model: 0.830805294691046
Bandwidth: 770
R2 of Local Model: 0.831274836665161
Bandwidth: 771
R2 of Local Model: 0.830376397878408
Bandwidth: 772
R2 of Local Model: 0.832604789981756
Bandwidth: 773
R2 of Local Model: 0.829576028459974
Bandwidth: 774
R2 of Local Model: 0.830796112836969
Bandwidth: 775
R2 of Local Model: 0.831481002981783
Bandwidth: 776
R2 of Local Model: 0.831676908866007
Bandwidth: 777
R2 of Local Model: 0.82931315042137
Bandwidth: 778
R2 of Local Model: 0.828951603116439
Bandwidth: 779
R2 of Local Model: 0.831139055149746
Bandwidth: 780
R2 of Local Model: 0.830316739537019
Bandwidth: 781
R2 of Local Model: 0.830458930586156
Bandwidth: 782
R2 of Local Model: 0.833230662394588
Bandwidth: 783
R2 of Local Model: 0.832973909576033
Bandwidth: 784
R2 of Local Model: 0.831423925116023
Bandwidth: 785
R2 of Local Model: 0.829803683418786
Bandwidth: 786
R2 of Local Model: 0.828531421840785
Bandwidth: 787
R2 of Local Model: 0.830345759971399
Bandwidth: 788
R2 of Local Model: 0.830471339400124
Bandwidth: 789
R2 of Local Model: 0.832798840438387
Bandwidth: 790
R2 of Local Model: 0.83113249780185
Bandwidth: 791
R2 of Local Model: 0.827034393025223
Bandwidth: 792
R2 of Local Model: 0.829133703096098
Bandwidth: 793
R2 of Local Model: 0.831933812229025
Bandwidth: 794
R2 of Local Model: 0.830849224075565
Bandwidth: 795
R2 of Local Model: 0.830321144305101
Bandwidth: 796
R2 of Local Model: 0.829138641042415
Bandwidth: 797
R2 of Local Model: 0.830608786109532
Bandwidth: 798
R2 of Local Model: 0.831307623612618
Bandwidth: 799
R2 of Local Model: 0.833242600852963
Bandwidth: 800
R2 of Local Model: 0.832427578593409
Bandwidth: 801
R2 of Local Model: 0.83096960265283
Bandwidth: 802
R2 of Local Model: 0.8301887673541
Bandwidth: 803
R2 of Local Model: 0.834255445238142
Bandwidth: 804
R2 of Local Model: 0.832521588463182
Bandwidth: 805
R2 of Local Model: 0.831715300054422
Bandwidth: 806
R2 of Local Model: 0.831639594906041
Bandwidth: 807
R2 of Local Model: 0.83115606975791
Bandwidth: 808
R2 of Local Model: 0.833148579276595
Bandwidth: 809
R2 of Local Model: 0.831414942555648
Bandwidth: 810
R2 of Local Model: 0.833719784150672
Bandwidth: 811
R2 of Local Model: 0.828375723575394
Bandwidth: 812
R2 of Local Model: 0.830543845828127
Bandwidth: 813
R2 of Local Model: 0.832240311900002
Bandwidth: 814
R2 of Local Model: 0.831434839902126
Bandwidth: 815
R2 of Local Model: 0.828803556504407
Bandwidth: 816
R2 of Local Model: 0.829547281428623
Bandwidth: 817
R2 of Local Model: 0.831058937941818
Bandwidth: 818
R2 of Local Model: 0.82938891796316
Bandwidth: 819
R2 of Local Model: 0.830405198798365
Bandwidth: 820
R2 of Local Model: 0.832115866093099
Bandwidth: 821
R2 of Local Model: 0.830412735051787
Bandwidth: 822
R2 of Local Model: 0.828572195678903
Bandwidth: 823
R2 of Local Model: 0.830266787825573
Bandwidth: 824
R2 of Local Model: 0.83184289974954
Bandwidth: 825
R2 of Local Model: 0.829420782829894
Bandwidth: 826
R2 of Local Model: 0.831359754548764
Bandwidth: 827
R2 of Local Model: 0.830368557576751
Bandwidth: 828
R2 of Local Model: 0.832429361513429
Bandwidth: 829
R2 of Local Model: 0.829330567671406
Bandwidth: 830
R2 of Local Model: 0.833664185213611
Bandwidth: 831
R2 of Local Model: 0.829605680296404
Bandwidth: 832
R2 of Local Model: 0.833311778486311
Bandwidth: 833
R2 of Local Model: 0.830353478426326
Bandwidth: 834
R2 of Local Model: 0.82977377053556
Bandwidth: 835
R2 of Local Model: 0.829937703219876
Bandwidth: 836
R2 of Local Model: 0.831846806163151
Bandwidth: 837
R2 of Local Model: 0.829895075127843
Bandwidth: 838
R2 of Local Model: 0.833262019160109
Bandwidth: 839
R2 of Local Model: 0.829433660783203
Bandwidth: 840
R2 of Local Model: 0.827531432547631
Bandwidth: 841
R2 of Local Model: 0.82975712679373
Bandwidth: 842
R2 of Local Model: 0.832414617979685
Bandwidth: 843
R2 of Local Model: 0.830781131471392
Bandwidth: 844
R2 of Local Model: 0.829109261064835
Bandwidth: 845
R2 of Local Model: 0.832406223647478
Bandwidth: 846
R2 of Local Model: 0.834708366156922
Bandwidth: 847
R2 of Local Model: 0.833191760455706
Bandwidth: 848
R2 of Local Model: 0.831812936634274
Bandwidth: 849
R2 of Local Model: 0.832387913503154
Bandwidth: 850
R2 of Local Model: 0.833075377352126
Bandwidth: 851
R2 of Local Model: 0.830123177762137
Bandwidth: 852
R2 of Local Model: 0.831841299216412
Bandwidth: 853
R2 of Local Model: 0.831365701551992
Bandwidth: 854
R2 of Local Model: 0.830989278187555
Bandwidth: 855
R2 of Local Model: 0.834752901702268
Bandwidth: 856
R2 of Local Model: 0.832148011460522
Bandwidth: 857
R2 of Local Model: 0.830067314893585
Bandwidth: 858
R2 of Local Model: 0.83078531914217
Bandwidth: 859
R2 of Local Model: 0.830479327296697
Bandwidth: 860
R2 of Local Model: 0.834880676239004
Bandwidth: 861
R2 of Local Model: 0.833506472321838
Bandwidth: 862
R2 of Local Model: 0.830843082210954
Bandwidth: 863
R2 of Local Model: 0.830132630677842
Bandwidth: 864
R2 of Local Model: 0.828704486734575
Bandwidth: 865
R2 of Local Model: 0.8293732367072
Bandwidth: 866
R2 of Local Model: 0.829996086927068
Bandwidth: 867
R2 of Local Model: 0.83288312812145
Bandwidth: 868
R2 of Local Model: 0.831467621740344
Bandwidth: 869
R2 of Local Model: 0.829259249144337
Bandwidth: 870
R2 of Local Model: 0.830770774516278
Bandwidth: 871
R2 of Local Model: 0.828837075909044
Bandwidth: 872
R2 of Local Model: 0.829758111980732
Bandwidth: 873
R2 of Local Model: 0.829557855443734
Bandwidth: 874
R2 of Local Model: 0.830523982844196
Bandwidth: 875
R2 of Local Model: 0.831009886895258
Bandwidth: 876
R2 of Local Model: 0.82944013294738
Bandwidth: 877
R2 of Local Model: 0.832028294836151
Bandwidth: 878
R2 of Local Model: 0.833992472427127
Bandwidth: 879
R2 of Local Model: 0.828809446103142
Bandwidth: 880
R2 of Local Model: 0.827440566241794
Bandwidth: 881
R2 of Local Model: 0.83157952544884
Bandwidth: 882
R2 of Local Model: 0.829294634223024
Bandwidth: 883
R2 of Local Model: 0.832394559788004
Bandwidth: 884
R2 of Local Model: 0.831141692432181
Bandwidth: 885
R2 of Local Model: 0.831071687513164
Bandwidth: 886
R2 of Local Model: 0.828715571246103
Bandwidth: 887
R2 of Local Model: 0.829535263164816
Bandwidth: 888
R2 of Local Model: 0.831319756325194
Bandwidth: 889
R2 of Local Model: 0.831550952889957
Bandwidth: 890
R2 of Local Model: 0.829990162602387
Bandwidth: 891
R2 of Local Model: 0.834388880202844
Bandwidth: 892
R2 of Local Model: 0.831444356778861
Bandwidth: 893
R2 of Local Model: 0.829154145538292
Bandwidth: 894
R2 of Local Model: 0.831118924732515
Bandwidth: 895
R2 of Local Model: 0.832741630646768
Bandwidth: 896
R2 of Local Model: 0.828300969146387
Bandwidth: 897
R2 of Local Model: 0.831320866320787
Bandwidth: 898
R2 of Local Model: 0.828244750811946
Bandwidth: 899
R2 of Local Model: 0.832772387332717
Bandwidth: 900
R2 of Local Model: 0.828538098204589
Bandwidth: 901
R2 of Local Model: 0.82750095108753
Bandwidth: 902
R2 of Local Model: 0.828826070773062
Bandwidth: 903
R2 of Local Model: 0.830187793236362
Bandwidth: 904
R2 of Local Model: 0.83278488113049
Bandwidth: 905
R2 of Local Model: 0.830157433907511
```

Base on the output, we will pick the best bandwidth that has the largest R2 of Local Model.
The best bandwidth would be 860 with a R2 of 0.834880676239004.

## Calibrating Geographical Random Forest Model

We will use grf() from the SpatialML package to calibrate our GRF Model.
It takes in the formula which is the same as the non-spatial regression.
We will use the bandwidth calculated from the above section and generate base on ntree =30.

```{r, eval = FALSE}
set.seed(1234)
gwRF_adaptive <- grf(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                     dframe=train_data_nogeo, 
                     bw= 860,
                     ntree = 30,
                     kernel="adaptive",
                     coords=coords_train)
```

Let's save the model output below:

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(gwRF_adaptive, "data/model/grf_adaptive_30.rds")
```

```{r}
gwRF_adaptive <- read_rds("data/model/grf_adaptive_30.rds")
```

```{r}
glimpse(gwRF_adaptive$Local.Variable.Importance)
gwRF_adaptive$Global.Model
```

## Predicting by using test data

### Preparing the test data

The code chunk below will be used to combine the test data with its corresponding coordinates data

```{r}
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

### Predicting with test data

Next, predict.grf() of SpatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r, eval = FALSE}
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

We should always save our output into rds since these computations take a really long time.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show code"
write_rds(gwRF_pred, "data/model/GRF_pred.rds")
```

### Converting predicted output into a data frame

```{r}
#reads the data in and makes it a dataframe
GRF_pred_df <- as.data.frame(read_rds("data/model/GRF_pred.rds"))
```

Then, we will use cbind() to append the predicted values onto the test data

```{r, eval = FALSE}
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r, eval = FALSE}
write_rds(test_data_p, "data/model/test_data_p.rds")
```

```{r}
test_data_p <- read_rds("data/model/test_data_p.rds")
```

### Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis.
In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

```{r}
glimpse(test_data_p)
```

We notice that the column name is stated `read_rds("data/model/GRF_pred.rds")`, we will change the name accordingly.

```{r}
test_data_p <- test_data_p %>%
  rename(GRF_pred = `read_rds("data/model/GRF_pred.rds")`)
```

```{r}
rmse_grf<- rmse(test_data_p$PRICE, 
     test_data_p$GRF_pred)
rmse_grf
```

We notice from the rmse is 56235.78, which is lower than the OLS rmse.
Furthermore, Rsquare of the gwRF is 0.91 which is much higher than the OLS model.
Hence, this is a better model in terms if its prediction accuracy.

### Visualising the predicted values

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = PRICE)) +
  geom_point()
```

# Conclusion

Lastly, we were task to compare the performance of the conventional OLS method versus the geographical weighted methods.

## Differences between the two models

1.  OLS does not take into account the spatial autocorrelation whereas GRF does.
2.  Conventional OLS is generally good for exploratory data analysis but even so, more needs to be done to improve the model if it is meant to be used for prediction

We will compare the two models base on RMSE and on the visual plot.

::: panel-tabset
## Non-Spatial Multiple linear Regression

```{r}
rmse_lm
```

```{r}
test_data_lm <- cbind(test_data, lm_predicted_value)
```

```{r}
ggplot(data = test_data_lm,
       aes(x = lm_predicted_value,
           y = PRICE)) +
  geom_point() +
  geom_abline(col = "Red")
```

## Geographical Random Forest Model

```{r}
rmse_grf
```

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = PRICE)) +
  geom_point()+
  geom_abline(col = "Red")
```
:::

## Predictive Power

By comparing the two models, we can see the RMSE for GRF, 56235.78, is lower than the RMSE for non-spatial regression, 87822.93.
This suggest that the GRF Model is better at predicting the resale price.

Furthermore, we also notice that the points scattered in the GRF Model are clustered around the red line more than the non-spatial regression.

## Limitations of Analysis

1.  One obvious limitation is the "sub-optimal" bandwidth generated for the GRF Model. This is due to the lack of computational power our devices have.
2.  Second is the number of trees that can be generated. Again for the same reason, I was only able to do ntree = 30. Having more trees may have better predictive power.

## Possible Improvements to the model

One way to improve the model could be the use of interactive variables.

Interactive variables helps to showcase the combined effects between two variables.
For example, story_ordinal and NUM of primary school as an interactive variable.
The intuition behind this is that primary schools generally contribute to higher noise-pollution and the higher the story level, the better as it can reduce the amount of noise that reaches the apartment.
Such combined effects may improve the predictive power the model have.

Another example could be PROX_Parks and Story_Order.
The closer one is to parks may signify better views.
Higher story may allow residents to have better views and thus raise the price of the apartment.
Again this is a useful variable to consider in our modelling.

# References

-   [Megan's work](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/)

Much was referenced to Megan's work on how to wrangle the data.
Her work revolves around create hedonic model and it is a good read to understand how she goes through her assignment.

-   [In-class Exercise 9](https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex09/in-class_ex09_gwml)

Prof Kam taught us several techniques on how to create the non-spatial regression as well as doing up the GRF modelling.
