---
title: "Take-home Exercise 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods"
date: "6 March 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# Setting the Scene

Housing is an essential component of household wealth worldwide.
Buying a housing has always been a major investment for most people.
The price of housing is affected by many factors.
Some of them are global in nature such as the general economy of a country or inflation rate.
Others can be more specific to the properties themselves.
These factors can be further divided to structural and locational factors.
Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property.
Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method.
However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions.
With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998).
In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## The Task

In this take-home exercise, you are tasked to predict HDB resale prices at the sub-market level (i.e.Â HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore.
The predictive models must be built by using by using conventional OLS method and GWR methods.
You are also required to compare the performance of the conventional OLS method versus the geographical weighted methods.

## Packages used:

```{r}
pacman::p_load(tidyverse, sf, sfdep, tmap, httr, jsonlite, matrixStats, readr, readxl, GWmodel, SpatialML, rsample, Metrics)
```

## Data used:

```{r}
# initialise a dataframe of our aspatial and geospatial dataset details
datasets <- data.frame(
  Type=c("Aspatial",
         "Geospatial",
         "Geospatial",
         "Geospatial",
         "Geospatial",
         
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         "Geospatial - Extracted",
         
         "Geospatial - Selfsourced",
         "Geospatial - Selfsourced",
         "Geospatial - Selfsourced",
         "Geospatial - Selfsourced"),
  
  Name=c("Resale Flat Prices",
         "Singapore National Boundary",
         "Master Plan 2014 Subzone Boundary (Web)",
         "MRT & LRT Locations Aug 2021",
         "Bus Stop Locations Aug 2021",
         
         "Childcare Services",
         "Eldercare Services",
         "Hawker Centres",
         "Kindergartens",
         "Parks",
         "Supermarkets",
         "Primary Schools",
         
         "Community Health Assistance Scheme (CHAS) Clinics",
         "Integrated Screening Programme (ISP) Clinics",
         "Public, Private and Non-for-profit Hospitals",
         "Shopping Mall SVY21 Coordinates`"),
  
  Format=c(".csv", 
           ".shp", 
           ".shp", 
           ".shp", 
           ".shp",
           
           ".shp", 
           ".shp", 
           ".shp", 
           ".shp",
           ".shp", 
           ".shp",
           ".shp",
           
           ".kml",
           ".shp",
           ".xlsx",
           ".csv"),
  
  Source=c("[data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)",
           "[data.gov.sg](https://data.gov.sg/dataset/national-map-polygon)",
           "[data.gov.sg](https://data.gov.sg/dataset/master-plan-2014-subzone-boundary-web)",
           "[LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=Train)",
           "[LTA Data Mall](https://datamall.lta.gov.sg/content/datamall/en/search_datasets.html?searchText=bus%20stop)",
           
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           
           "[data.gov.sg](https://data.gov.sg/dataset/chas-clinics)",
           "[OneMap API](https://www.onemap.gov.sg/docs/)",
           "Self-sourced and collated (section 2.3)",
           "[Mall SVY21 Coordinates Web Scaper](https://github.com/ValaryLim/Mall-Coordinates-Web-Scraper)")
  )

# with reference to this guide on kableExtra:
# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html
# kable_material is the name of the kable theme
# 'hover' for to highlight row when hovering, 'scale_down' to adjust table to fit page width
library(knitr)
library(kableExtra)
kable(datasets, caption="Datasets Used") %>%
  kable_material("hover", latex_options="scale_down")

```

### How to extract data from onemap

These are the steps:

*The code chunks below uses onemapsgapi package.*

1.  Register an account with [onemap](https://developers.onemap.sg/register/)
2.  A code is then be sent to your email. Then fill in this [form](https://developers.onemap.sg/confirm_account/)
3.  In the console, run the code chunk below:

```{r, eval = FALSE}
token <- get_token("email", "password")
```

Input the email and password that you've registered with onemap.
This will provide you a token ID under objectname: token.
Note that you will need to do this again as the token is only valid for 3 days.

4.  Obtain queryname by running the code chunk below:

```{r, eval = FALSE}
themes <- search_themes(token)
```

Input your token ID and you can source for the queryname for Step 5.

5.  Use get_theme() to get the data from onemap

For this example, we will use queryname, "eldercare".

```{r, eval = FALSE}
eldercare<-get_theme(token,"eldercare")
```

6.  Convert the object into an sf object then download it into your data folder. st_as_sf() is for converting the file and st_write() is for writing the file into your folder. st_transform() sets the coordinate reference system to Singapore, EPSG::3414. These functions are from the sf package.

```{r, eval = FALSE}
eldercaresf <- st_as_sf(eldercare, coords=c("Lng", "Lat"), 
                        crs=4326) %>% 
  st_transform(crs = 3414)
st_write(obj = eldercaresf,
         dsn = "data/geospatial",
         layer = "eldercare",
         driver = "ESRI Shapefile")
```

To make it more automatic, define which variables you want from the onemap database into a vector.
The code chunk runs a for loop that does steps 5 and 6 together and stores them into your folder.

```{r, eval = FALSE}
onemap_variables <- c("childcare", "communityclubs", "eldercare", "family", "hawkercentre", "kindergartens", "nationalparks","registered_pharmacy")

df <- list()
df_sf <- list()
for (i in 1:length(onemap_variables)){
  df[[i]] <- get_theme(token, onemap_variables[i])
  df_sf[[i]] <- st_as_sf(df[[i]], coords=c("Lng", "Lat"), 
                        crs=4326) %>%
    st_transform(crs = 3414)
st_write(obj = df_sf[[i]],
         dsn = "data/geospatial/Onemap",
         layer = onemap_variables[i],
         driver = "ESRI Shapefile")
}
```

# Load Data into R

## Geospatial Data

::: panel-tabset
### One-Map

```{r}
pharmacy <- st_read(dsn = "data/geospatial/Onemap",
                layer = "registered_pharmacy")
parks <- st_read(dsn = "data/geospatial/Onemap",
                 layer = "nationalparks")
kindergartens <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "kindergartens")
hawker <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "hawkercentre")
eldercare <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "eldercare")
communityclubs <-st_read(dsn = "data/geospatial/Onemap",
                    layer = "communityclubs")
supermarkets <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "SUPERMARKETS")
familyservices <- st_read(dsn = "data/geospatial/Onemap",
                    layer = "family")
childcare <- st_read(dsn = "data/geospatial/Onemap",
                     layer = "childcare")
```

### Geospatial Map

```{r}
mpsz <- st_read(dsn = "data/geospatial",
                layer = "MP14_SUBZONE_WEB_PL")
```

### DATA.GOV

```{r}
Bus_stop <- st_read(dsn = "data/geospatial",
                layer = "BusStop")
MRT <- st_read(dsn = "data/geospatial/lta-mrt-station-exit-kml.kml")
```

### Aspatial Data

```{r}
Resale <- read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
```

```{r}
Schools <- read_csv("data/aspatial/schools.csv")
```

```{r}
Malls <- read_csv("data/aspatial/mall_coordinates_updated.csv")
```
:::

# Data Wrangling

## HDB Resale Price

For the purpose of this take-home exercise, we will only be using five-room flat and transaction period should be January and February 2023.

From the output above, the variables we want are:

-   Resale Price
-   Month
-   Flat Type
-   Area of the unit
-   Floor level
-   Remaining Lease
-   Flat Model

### Story and Date adjustents

We will get the unique values of story by running

```{r}
unique(Resale$storey_range)
```

The code chunk below will sequence the story levels, assigning value 1 to "01 TO 03", 2 to "04 TO 06", ..., 17 to "49 TO 51".

```{r, eval = FALSE}
# Define the story levels and ordinal values
story_levels <- c("01 TO 03", "04 TO 06", "07 TO 09", "10 TO 12", "13 TO 15", "16 TO 18", "19 TO 21", "22 TO 24", "25 TO 27", "28 TO 30", "31 TO 33", "34 TO 36", "37 TO 39", "40 TO 42", "43 TO 45", "46 TO 48", "49 TO 51")
story_ordinal <- seq_along(story_levels)

# Create the ordinal variable based on the story column
Resale$Story_Ordinal <- story_ordinal[match(Resale$storey_range, story_levels)]

# Set the labels for the ordinal variable
levels(Resale$Story_Ordinal) <- story_levels
```

### Date

Next we will set the Date column as date type and also ensure Story_Ordinal is of numeric type.

```{r, eval = FALSE}
Resale <- Resale %>%
  mutate(month = as.Date(paste0(month, "-01"), format ="%Y-%m-%d"),
         Story_Ordinal = as.numeric(Story_Ordinal))
```

### Flat Model

Next, we will set Flat Model as a binary variable by running the code chunk below.

```{r, eval = FALSE}
Resale <- Resale %>%
  tidyr::pivot_wider(names_from = flat_model,
              values_from = flat_model, 
              values_fn = list(flat_model = ~1), 
              values_fill = 0)
```

### Remaining Lease

We will settle the remaining lease by running the code chunk below: This turns remaining_lease from chr type to numeric in terms of units, years.

```{r, eval = FALSE}
# Splits the string by year and month, using str_split from the stringr package
str_list <- str_split(Resale$remaining_lease, " ")

for (i in 1:length(str_list)) {
  if (length(unlist(str_list[i])) > 2) {
      year <- as.numeric(unlist(str_list[i])[1])
      month <- as.numeric(unlist(str_list[i])[3])
      Resale$remaining_lease[i] <- year + round(month/12, 2)
  }
  else {
    year <- as.numeric(unlist(str_list[i])[1])
    Resale$remaining_lease[i] <- year
  }
}

Resale <- Resale %>%
  mutate(remaining_lease =as.numeric(remaining_lease))
```

We can take a look at the dataframe now by using glimpse() from the dplyr package

```{r}
glimpse(Resale)
```

We will first filter out the relevant columns we want by running the code chunk below:

select() helps to select the columns we want and filter() helps us to filter to the specific two months.
These two functions come from the dplyr package.

::: panel-tabset
### Training Data

```{r, eval = FALSE}
Resale_train <- Resale %>%
  filter(month >= "2021-01-01" & month <= "2022-12-01",
         flat_type == "5 ROOM") %>%
  dplyr::select(-2,-3,-6,-8)
```

We will save Resale_train as a rds file for easy retrieval

```{r, eval = FALSE}
write_rds(Resale_train, "data/aspatial/Resale_train.rds")
```

```{r}
Resale_train <- read_rds("data/aspatial/Resale_train.rds")
```

```{r}
glimpse(Resale_train)
```

### Test Data

```{r, eval = FALSE}
Resale_test <- Resale %>%
  filter(month >="2023-01-01" & month <="2023-02-01",
         flat_type == "5 ROOM") %>%
  dplyr::select(-2,-3,-6,-8)
```

We will save Resale_train as a rds file for easy retrieval

```{r, eval = FALSE}
write_rds(Resale_test, "data/aspatial/Resale_test.rds")
```

```{r}
Resale_test <- read_rds("data/aspatial/Resale_test.rds")
```

```{r}
glimpse(Resale_test)
```
:::

### Inserting geometries

To do this, we will have to obtain the geometries from this [url](https://developers.onemap.sg/commonapi/search).
This code is referenced from [Megan's work](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/).

```{r, eval = FALSE}
library("httr")
geocode <- function(block, streetname) {
  base_url <- "https://developers.onemap.sg/commonapi/search"
  address <- paste(block, streetname, sep = " ")
  query <- list("searchVal" = address, 
                "returnGeom" = "Y",
                "getAddrDetails" = "N",
                "pageNum" = "1")
  
  res <- GET(base_url, query = query)
  restext<-content(res, as="text")
  
  output <- jsonlite::fromJSON(restext)  %>% 
    as.data.frame() %>%
    dplyr::select("results.LATITUDE", "results.LONGITUDE")
  return(output)
}
```

::: panel-tabset
#### Training Data

```{r, eval = FALSE}
Resale_train$LATITUDE <- 0
Resale_train$LONGITUDE <- 0

for (i in 1:nrow(Resale_train)){
  temp_output <- geocode(Resale_train[i, 2], Resale_train[i, 3])
  
  Resale_train$LATITUDE[i] <- temp_output$results.LATITUDE
  Resale_train$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

#### Testing Data

```{r, eval = FALSE}
Resale_test$LATITUDE <- 0
Resale_test$LONGITUDE <- 0

for (i in 1:nrow(Resale_test)){
  temp_output <- geocode(Resale_test[i, 2], Resale_test[i, 3])
  
  Resale_test$LATITUDE[i] <- temp_output$results.LATITUDE
  Resale_test$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```
:::

### Convert into Resale_2023 dataframe into sf

By using st_as_sf() from the sf package, we can convert Resale_2023 into an sf and then transform the crs to EPSG:: 3414 which is the coordinate reference system for Singapore

::: panel-tabset
#### Training Data

```{r, eval = FALSE}
Resale_training_sf <- st_as_sf(Resale_train, 
                      coords = c("LONGITUDE", "LATITUDE"),
                      crs = 4326) %>%
  st_transform(crs = 3414)
```

We can store it as a shapefile for future retrieval.

```{r,eval = FALSE}
st_write(Resale_training_sf, 
         dsn = "data/aspatial", 
         layer = "resale_train_sf",
         driver = "ESRI Shapefile")
```

```{r}
Resale_training_sf <- st_read(dsn = "data/aspatial",
                     layer = "resale_train_sf")
```

#### Test Data

```{r, eval = FALSE}
Resale_test_sf <- st_as_sf(Resale_test, 
                      coords = c("LONGITUDE", "LATITUDE"),
                      crs = 4326) %>%
  st_transform(crs = 3414)
```

We can store it as a shapefile for future retrieval

```{r,eval = FALSE}
st_write(Resale_test_sf, 
         dsn = "data/aspatial", 
         layer = "Resale_test_sf",
         driver = "ESRI Shapefile")
```

```{r}
Resale_test_sf <- st_read(dsn = "data/aspatial",
                     layer = "Resale_test_sf")
```
:::

## Aspatial Data Wrangling

For schools, the dataframe holds several levels of schools.
For the purpose the regression, we will focus only on primary and secondary schools.

```{r}
unique(Schools$mainlevel_code)
```

We will create the respective primary school and secondary school dataframe.

We will also make some tweaks to the geocode function created above, that takes in postal code instead of block and street.

```{r}
geocode <- function(postal) {
  base_url <- "https://developers.onemap.sg/commonapi/search"
  query <- list("searchVal" = postal, 
                "returnGeom" = "Y",
                "getAddrDetails" = "N",
                "pageNum" = "1")
  
  res <- GET(base_url, query = query)
  restext<-content(res, as="text")
  
  output <- jsonlite::fromJSON(restext)  %>% 
    as.data.frame() %>%
    dplyr::select("results.LATITUDE", "results.LONGITUDE")
  return(output)
}
```

::: panel-tabset
### Primary School

```{r}
Primary <- Schools %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name,postal_code)
```

```{r, eval = FALSE}
Primary$LATITUDE <- 0
Primary$LONGITUDE <- 0

for (i in 1:nrow(Primary)){
  temp_output <- geocode(Primary[i, 2])
  
  Primary$LATITUDE[i] <- temp_output$results.LATITUDE
  Primary$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

```{r, eval =FALSE}
write_rds(Primary, "data/aspatial/Primary.rds")
```

```{r}
Primary <- read_rds("data/aspatial/Primary.rds")
```

### Secondary School

```{r, eval = FALSE}
Secondary <- Schools %>%
  filter(mainlevel_code == "SECONDARY") %>%
  select(school_name,postal_code)
```

After running the geocode loop on secondary school, we notice that [ZHENGHUAÂ SECONDARYÂ SCHOOL](https://www.moe.gov.sg/schoolfinder/schooldetail?schoolname=zhenghua-secondary-school) has a wrong postal code.

```{r, eval = FALSE}
Secondary[137,2]<- "679962"
```

We can rerun the code.

```{r, eval = FALSE}
Secondary$LATITUDE <- 0
Secondary$LONGITUDE <- 0

for (i in 1:nrow(Secondary)){
  temp_output <- geocode(Secondary[i, 2])
  
  Secondary$LATITUDE[i] <- temp_output$results.LATITUDE
  Secondary$LONGITUDE[i] <- temp_output$results.LONGITUDE
}
```

```{r, eval = FALSE}
write_rds(Secondary, file = "data/aspatial/Secondary.rds")
```

```{r}
Secondary <- read_rds("data/aspatial/Secondary.rds")
```
:::

For Primary_sch, Secondary_sch and Mall, we need to convert them into sf and transform it to the correct CRS.

Primary School:

```{r}
Primary_sf <- Primary %>%
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"),
           crs = 4326) %>% 
  st_transform(crs = 3414)
```

Good Primary Schools:

We will refer to [schlah](https://schlah.com/primary-schools) list of good primary schools.
Note that CANOSSA HIGH SCHOOL in schlah webpage is known as CANOSSA CATHOLIC PRIMARY SCHOOL.

```{r}
Good_Prisch <- Primary_sf %>%
  filter(school_name %in% c("NANYANG PRIMARY SCHOOL",
                            "TAO NAN SCHOOL",
                            "CANOSSA CATHOLIC PRIMARY SCHOOL",
                            "NAN HUA PRIMARY SCHOOL",
                            "ST. HILDA'S PRIMARY SCHOOL",
                            "HENRY PARK PRIMARY SCHOOL",
                            "ANGLO-CHINESE SCHOOL (PRIMARY)",
                            "RAFFLES GIRLS' PRIMARY SCHOOL",
                            "PEI HWA PRESBYTERIAN PRIMARY SCHOOL"
                            ))
```

Secondary School:

```{r}
Secondary_sf <- Secondary %>%
  st_as_sf(coords = c("LONGITUDE","LATITUDE"),
           crs = 4326) %>% 
  st_transform(crs = 3414)
```

Malls:

```{r}
malls <- Malls %>%
  st_as_sf(coords = c("longitude", "latitude"),
           crs = 4326) %>%
  st_transform(crs= 3414)
```

CBD Area:

```{r}
lat <- 1.287953
lng <- 103.851784

cbd_sf <- data.frame(lat, lng) %>%
  st_as_sf(coords = c("lng", "lat"), crs=4326) %>%
  st_transform(crs=3414)
```

## Ensure all datasets are in EPSG:3414

Aside from the onemap variables, we need to check the geospatial data and the other data sets, primary school, secondary school and shopping malls.

```{r}
st_crs(mpsz) ; st_crs(Bus_stop) ; st_crs(MRT) ; st_crs(supermarkets)
```

We notice that the CRS for these 3 geospatial data is not correct.
We will use st_transformed as taught previously to convert it to EPSG:3414

```{r}
mpsz <- mpsz %>%
  st_transform(crs = 3414)
```

```{r}
Bus_stop <- Bus_stop %>%
  st_transform(crs = 3414)
```

```{r}
MRT <- MRT %>%
  st_transform(crs = 3414)
```

```{r}
supermarkets <- supermarkets %>%
  st_transform(crs = 3414)
```

## Checking invalid geometries

We should also check for any invalid geometries

::: panel-tabset
### Onemap variable checks

```{r}
length(which(st_is_valid(communityclubs)== FALSE))
```

```{r}
length(which(st_is_valid(eldercare)== FALSE))
```

```{r}
length(which(st_is_valid(familyservices)== FALSE))
```

```{r}
length(which(st_is_valid(hawker)== FALSE))
```

```{r}
length(which(st_is_valid(kindergartens)== FALSE))
```

```{r}
length(which(st_is_valid(parks)== FALSE))
```

```{r}
length(which(st_is_valid(pharmacy)== FALSE))
```

```{r}
length(which(st_is_valid(supermarkets)== FALSE))
```

```{r}
length(which(st_is_valid(childcare)== FALSE))
```

We can see that for the oneMap variables, there are no invalid geometries.

### DATA.GOV

```{r}
length(which(st_is_valid(mpsz)== FALSE))
```

```{r}
length(which(st_is_valid(Bus_stop)== FALSE))
```

```{r}
length(which(st_is_valid(MRT)== FALSE))
```

For the data obtained from DATA.GOV, there are some invalid geometries

We will proceed to remove them

```{r}
mpsz <- st_make_valid(mpsz)
```

### Aspatial Data

```{r}
length(which(st_is_valid(Primary_sf)== FALSE))
```

```{r}
length(which(st_is_valid(Secondary_sf)== FALSE))
```

```{r}
length(which(st_is_valid(malls)==FALSE))
```

There is no invalid geometries in the aspatial data set.
:::

## Removing Unnecessary columns

::: panel-tabset
### Onemap

```{r}
communityclubs <- communityclubs %>%
  select(1)
```

```{r}
eldercare <- eldercare %>%
  select(1)
```

```{r}
familyservices <- familyservices %>%
  select(1)
```

```{r}
hawker <- hawker %>%
  select(1)
```

```{r}
kindergartens <- kindergartens %>%
  select(1)
```

```{r}
parks <- parks %>%
  select(1)
```

```{r}
pharmacy <- pharmacy %>%
  select(1)
```

```{r}
supermarkets <- supermarkets %>%
  select(1)
```

```{r}
childcare <- childcare %>%
  select(1)
```

### DATA.GOV

```{r}
Bus_stop <- select(Bus_stop, 1)
```

For MRT, we will need to drop the Z-dimension.
This can be seen when you View(MRT).

```{r}
MRT_Station <- st_zm(MRT) %>%
  select(1)
```

### Aspatial Data

```{r}
Primary_sf <- select(Primary_sf, 1)
```

```{r}
Secondary_sf <- select(Secondary_sf, 1)
```

```{r}
malls <- select(malls, name)
```
:::

## Check NA values

::: panel-tabset
### Onemap

```{r}
communityclubs[rowSums(is.na(communityclubs))!=0,]
```

```{r}
eldercare[rowSums(is.na(eldercare))!=0,]
```

```{r}
familyservices[rowSums(is.na(familyservices))!=0,]
```

```{r}
hawker[rowSums(is.na(hawker))!=0,]
```

```{r}
kindergartens[rowSums(is.na(kindergartens))!=0,]
```

```{r}
parks[rowSums(is.na(parks))!=0,]
```

```{r}
pharmacy[rowSums(is.na(pharmacy))!=0,]
```

```{r}
supermarkets[rowSums(is.na(supermarkets))!=0,]
```

```{r}
childcare[rowSums(is.na(childcare))!=0,]
```

We notice that family services has NA values, we will proceed to remove them using na.omit()

```{r}
familyservices<- na.omit(familyservices, c("ADDRESSBUI"))
```

### DATA.GOV

```{r}
Bus_stop[rowSums(is.na(Bus_stop))!=0,]
```

```{r}
MRT_Station[rowSums(is.na(MRT_Station))!=0,]
```

```{r}
mpsz[rowSums(is.na(mpsz))!=0,]
```

### Aspatial Data

```{r}
length(which(is.na(Primary_sf) == TRUE))
```

```{r}
length(which(is.na(Secondary_sf) == TRUE))
```

```{r}
length(which(is.na(malls) == TRUE))
```
:::

# Visualisation

## Initial Mapping

::: panel-tabset
### Subzone

```{r}
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons()
```

### MRT Stations

```{r, eval = FALSE}
# Error
tmap_mode("plot")
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(MRT_Station) +
  tm_dots()
```

### Bus Stops

```{r}
tmap_mode("plot")
tm_shape(mpsz) + 
  tm_polygons() +
tm_shape(Bus_stop) +
  tm_dots(col = "red",
          size = 0.0075)
```

Note that the points outside of Singapore boundaries are related to bus stop at Johor Bahru and they are valid points to our analysis.

### Community Club

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(communityclubs) +
  tm_dots()
```

### Eldercare

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(eldercare) +
  tm_dots()
```

### Family Service

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(familyservices) +
  tm_dots()
```

### Hawker Centre

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(hawker) +
  tm_dots()
```

### Kindergarten

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(kindergartens) +
  tm_dots()
```

### Malls

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(malls) +
  tm_dots()
```

### Parks

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(parks) +
  tm_dots()
```

### Pharmacies

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(pharmacy) +
  tm_dots()
```

### Childcare

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(childcare) +
  tm_dots()
```

### Primary School

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(Primary_sf) +
  tm_dots()
```

### Secondary School

```{r}
tm_shape(mpsz) +
  tm_polygons() +
tm_shape(Secondary_sf) +
  tm_dots()
```
:::

# Proximity distance calculator

```{r, eval = FALSE}
library(units)
library(matrixStats)
proximity <- function(df1, df2, varname) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units()
  df1[,varname] <- rowMins(dist_matrix)
  return(df1) }
```

::: panel-tabset
## Training Data

```{r, eval = FALSE}
training_resale <- 
  # the columns will be truncated later on when viewing 
  # so we're limiting ourselves to two-character columns for ease of viewing between
  proximity(Resale_training_sf, cbd_sf, "PROX_CBD") %>%
  proximity(.,Bus_stop, "PROX_BUS") %>%
  proximity(., communityclubs, "PROX_CLUBS") %>%
  proximity(., eldercare, "PROX_ELDERCARE") %>%
  proximity(., familyservices, "PROX_FAM") %>%
  proximity(., MRT_Station, "PROX_MRT") %>%
  proximity(., hawker, "PROX_HAWKER") %>%
  proximity(., kindergartens, "PROX_KINDERGARTENS") %>%
  proximity(., pharmacy, "PROX_PHARMACY") %>%
  proximity(., parks, "PROX_PARK") %>%
  proximity(., malls, "PROX_MALL") %>%
  proximity(., supermarkets, "PROX_SPRMKT") %>%
  proximity(., childcare, "PROX_CHILDCARE") %>%
  proximity(., Primary_sf, "PROX_PRISCH") %>%
  proximity(., Good_Prisch, "PROX_GOODP") %>%
  proximity(., Secondary_sf, "PROX_SECSCH") 
```

```{r, eval =FALSE}
training_resale <- training_resale %>%
  mutate() %>%
  rename("AREA_SQM" = "flr_r_s",
         "PRICE" = "rsl_prc",
         "REMAINING_LEASE" = "rmnng_l") %>%
  relocate(`PRICE`)
```

## Test Data

```{r, eval=FALSE}
test_resale <- 
  # the columns will be truncated later on when viewing 
  # so we're limiting ourselves to two-character columns for ease of viewing between
  proximity(Resale_test_sf, cbd_sf, "PROX_CBD") %>%
  proximity(.,Bus_stop, "PROX_BUS") %>%
  proximity(., communityclubs, "PROX_CLUBS") %>%
  proximity(., eldercare, "PROX_ELDERCARE") %>%
  proximity(., familyservices, "PROX_FAM") %>%
  proximity(., MRT_Station, "PROX_MRT") %>%
  proximity(., hawker, "PROX_HAWKER") %>%
  proximity(., kindergartens, "PROX_KINDERGARTENS") %>%
  proximity(., pharmacy, "PROX_PHARMACY") %>%
  proximity(., parks, "PROX_PARK") %>%
  proximity(., malls, "PROX_MALL") %>%
  proximity(., supermarkets, "PROX_SPRMKT") %>%
  proximity(., childcare, "PROX_CHILDCARE") %>%
  proximity(., Primary_sf, "PROX_PRISCH") %>%
  proximity(., Good_Prisch, "PROX_GOODP") %>%
  proximity(., Secondary_sf, "PROX_SECSCH") 
```

```{r, eval = FALSE}
test_resale <- test_resale %>%
  mutate() %>%
  rename("AREA_SQM" = "flr_r_s",
         "PRICE" = "rsl_prc",
         "REMAINING_LEASE" = "rmnng_l") %>%
  relocate(`PRICE`)
```
:::

## Facility Count within Radius Calculation

Here, we want to find the number of facilities within a particular radius.
Like above, we'll use st_distance() to compute the distance between the flats and the desired facilities, and then sum up the observations with rowSums().
The values will be appended to the data frame as a new column.

```{r, eval = FALSE}
num_radius <- function(df1, df2, varname, radius) {
  dist_matrix <- st_distance(df1, df2) %>%
    drop_units() %>%
    as.data.frame()
  df1[,varname] <- rowSums(dist_matrix <= radius)
  return(df1)
}
```

::: panel-tabset
### Training Data

```{r, eval = FALSE}
training_resale <- 
  num_radius(training_resale, kindergartens, "NUM_KNDRGTN", 350) %>%
  num_radius(., childcare, "NUM_CHILDCARE", 350) %>%
  num_radius(., Bus_stop, "NUM_BUS_STOP", 350) %>%
  num_radius(., Primary_sf, "NUM_PRISCH", 1000) %>%
  num_radius(., Secondary_sf, "NUM_SECSCH", 1000)
```

```{r, eval = FALSE}
write_rds(training_resale, "data/aspatial/training_resale.rds")
```

### Test Data

```{r, eval = FALSE}
test_resale <- 
  num_radius(test_resale, kindergartens, "NUM_KNDRGTN", 350) %>%
  num_radius(., childcare, "NUM_CHILDCARE", 350) %>%
  num_radius(., Bus_stop, "NUM_BUS_STOP", 350) %>%
  num_radius(., Primary_sf, "NUM_PRISCH", 1000) %>%
  num_radius(., Secondary_sf, "NUM_SECSCH", 1000)
```

```{r, eval = FALSE}
write_rds(test_resale, "data/aspatial/test_resale.rds")
```
:::

# Read Data in

```{r}
training_data <- read_rds("data/aspatial/training_resale.rds")
test_data <- read_rds("data/aspatial/test_resale.rds")
```

::: panel-tabset
### Training Data

```{r}
glimpse(training_data)
```

We can see that we have forgotten to remove Block and street.
We can also check and confirm that all variables are in the right format.

```{r}
training_data <- training_data %>%
  select(-block, -strt_nm)
```

### Test Data

```{r}
glimpse(test_data)
```

We can see that we have forgotten to remove Block and street.
We can also check and confirm that all variables are in the right format.

```{r}
test_data <- test_data %>%
  select(-block, -strt_nm)
```
:::

# Computing Correlation Matrix

```{r}
training_data_nogeo <- training_data %>%
  st_drop_geometry()
```

We will first create the correlation matrix and check for any NA or infinite values.

```{r}
cor_matrix <- cor(training_data_nogeo[,3:47])
any(is.na(cor_matrix)) # check for missing values
any(is.infinite(cor_matrix)) # check for infinite values
```

Since there are missing values, we will fix this by assigning 0 to them

```{r}
na_value <- is.na(cor_matrix)
cor_matrix[na_value] <- 0
```

```{r}
corrplot::corrplot(cor_matrix, 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.2, 
                   method = "number", 
                   type = "upper")
```

We can see several correlated variables above 0.5 but they are within acceptable range to be included in the regression.

# Building a non-spatial multiple linear regression

```{r}
price_mlr <- lm(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + NwGnrtn + DBSS + Standrd + Aprtmnt+ Simplfd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Maisntt + Type.S1 + Type.S2 + ModelA2 + Terrace + Imprv.M + PrmmMsn + MltGnrt + PrmmApL+ X2.room + X3Gen + PROX_CBD + PROX_BUS + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_KINDERGARTENS+ PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_CHILDCARE + PROX_PRISCH + PROX_GOODP + PROX_SECSCH + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = training_data)
summary(price_mlr)
```

We can see that there are 11 singularities.
This is because when we change from categorical to binary, we need to exclude them to ensure that there are no linear dependency.
We will exclude NwGnrtn, Aprtmnt, Simplfd, Maisntt, Type.S1, ModelA2, Terrace, PrmmMsn, MltGnrt, X2.room, X3Gen

```{r, eval = FALSE}
price_mlr1 <- lm(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = training_data)
summary(price_mlr1)
```

```{r, eval = FALSE}
write_rds(price_mlr, "data/model/price_mlr.rds")
```

```{r, eval = FALSE}
write_rds(price_mlr1, "data/model/price_mlr1.rds")
```

# GWR Predictive Method

## Converting sf dataframe to SpatialPointDataframe

```{r}
train_data_sp <- as_Spatial(training_data)
train_data_sp
```

```{r, eval = FALSE}
bw_adaptive <- bw.gwr(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

![](images/image-1232532573.png)

The smallest CV score is adaptive bandwidth: 1005.

```{r, eval = FALSE}
write_rds(bw_adaptive, file = "data/model/bw_adaptive.rds")
```

## Construct the adaptive bandwidth gwr

```{r}
bw_adaptive <- read_rds("data/model/bw_adaptive.rds")
bw_adaptive
```

```{r, eval = FALSE}
gwr_adaptive <- gwr.basic(formula = PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
```

```{r, eval = FALSE}
write_rds(gwr_adaptive, "data/model/gwr_adaptive.rds")
```

```{r}
gwr_adaptive <- read_rds("data/model/gwr_adaptive.rds")
gwr_adaptive
```

## Preparing coordinates data

### Extracting coordinates data

The code chunk below extract the x, y coordinates of the full, training and test data sets.
This is as taught in in-class exercise 9.

```{r, eval = FALSE}
coords_train <- st_coordinates(training_data)
coords_test <- st_coordinates(test_data)
```

Before we proceed, we should save the output into rds for future uses

```{r, eval = FALSE}
write_rds(coords_train, "data/model/coords_train.rds" )
write_rds(coords_test, "data/model/coords_test.rds" )
```

```{r}
coords_train <- read_rds("data/model/coords_train.rds")
coords_test <- read_rds("data/model/coords_test.rds")
```

### Dropping geometry field

We need to drop the geometry column of the sf data.frame by using st_drop_geometry() of the sf package.
This is because, in the later parts, we cannot have the geometry columns in the data when running the random forest model.

```{r, eval = FALSE}
train_data_nogeo <- training_data %>%
  st_drop_geometry()
```

```{r, eval = FALSE}
write_rds(train_data_nogeo, "data/model/train_data_nogeo.rds")
```

```{r}
train_data_nogeo <- read_rds("data/model/train_data_nogeo.rds")
```

## Calibrating Random Forest Model

The code chunk below uses ranger() from the ranger package

This is done to calibrate the model, for predicting HDB resale prices.

```{r}
set.seed(1234)
rf <- ranger(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_nogeo)
```

```{r}
print(rf)
```

```{r, eval = FALSE}
gwRF_bw <- grf.bw(formula = PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                data = train_data_nogeo,
                kernel = "adaptive",
                coords = coords_train
                )
```

We will save the bandwidth output by using the code chunk below:

```{r, eval = FALSE}
write_rds(gwRF_bw, "data/model/gwRF_bw.rds")
```

## Calibrating Geographical Random Forest Model

```{r, eval = FALSE}
set.seed(1234)
gwRF_adaptive <- grf(PRICE ~ AREA_SQM + REMAINING_LEASE + Stry_Or + Improvd + DBSS + Standrd + Model.A + PrmmApr + Adjndfl + MdlA.Ms + Type.S2 + Imprv.M + PrmmApL + PROX_CBD + PROX_CLUBS + PROX_ELDERCARE + PROX_FAM + PROX_MRT +PROX_HAWKER + PROX_PHARMACY +PROX_PARK+PROX_MALL + PROX_SPRMKT + PROX_GOODP + NUM_KNDRGTN + NUM_CHILDCARE + NUM_BUS_STOP + NUM_PRISCH + NUM_SECSCH,
                     dframe=train_data_nogeo, 
                     bw= bw_adaptive,
                     ntree = 30,
                     kernel="adaptive",
                     coords=coords_train)
```

Let's save the model output below:

```{r, eval = FALSE}
write_rds(gwRF_adaptive, "data/model/gwRF_adaptive.rds")
```

```{r}
gwRF_adaptive <- read_rds("data/model/gwRF_adaptive.rds")
```

## Predicting by using test data

### Preparing the test data

The code chunk below will be used to combine the test data with its corresponding coordinates data

```{r}
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

### Predicting with test data

Next, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r, eval = FALSE}
gwRF_pred <- gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

We should always save our output into rds since these computations take a really long time.

```{r, eval = FALSE}
write_rds(gwRF_pred, "data/model/GRF_pred.rds")
```

### Converting predicted output into a data frame

```{r}
GRF_pred_df <- as.data.frame(read_rds("data/model/GRF_pred.rds"))
```

Then, we will use cbind() to append the predicted values onto the test data

```{r, eval = FALSE}
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r, eval = FALSE}
write_rds(test_data_p, "data/model/test_data_p.rds")
```

```{r}
test_data_p <- read_rds("data/model/test_data_p.rds")
```

### Calculating Root Mean Square Error

The root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis.
In the code chunk below, rmse() of Metrics package is used to compute the RMSE.

```{r}
glimpse(test_data_p)
```

We notice that the column name is stated `read_rds("data/model/GRF_pred.rds")`, we will change the name accordingly.

```{r}
test_data_p <- test_data_p %>%
  rename(GRF_pred = `read_rds("data/model/GRF_pred.rds")`)
```

```{r}
rmse(test_data_p$PRICE, 
     test_data_p$GRF_pred)
```

### Visualisaing the predicted values

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = PRICE)) +
  geom_point()
```
