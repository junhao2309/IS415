---
title: "Take-Home Exercise 2: Spatio-temporal analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jarkarta"
date: "13 February 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# Context

Since the outbreak of COVID-19, many countries have rushed to create a vaccine and ensure its population is well immunized against this novel coronavirus.
On 13 January 2021, the mass immunisation program commenced and since then Indonesia ranks third in Asia and fifth in the world in terms of total doses given.

In this take-home assignment, we will be exploring vaccination rates in DKI Jarkarta, identifying sub-districts with relatively higher number of vaccination rate and how they changed over time.

The tasks given to us is as follows:

**Choropleth Mapping and Analysis**

-   Compute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,

-   Prepare the monthly vaccination rate maps by using appropriate tmap functions,

-   Describe the spatial patterns revealed by the choropleth maps (not more than 200 words).

**Local Gi\* Analysis**

With reference to the vaccination rate maps prepared in ESDA:

-   Compute local Gi\* values of the monthly vaccination rate,

-   Display the Gi\* maps of the monthly vaccination rate.
    The maps should only display the significant (i.e. p-value \< 0.05)

-   With reference to the analysis results, draw statistical conclusions (not more than 250 words).

**Emerging Hot Spot Analysis(EHSA)**

With reference to the local Gi\* values of the vaccination rate maps prepared in the previous section:

-   Perform Mann-Kendall Test by using the spatio-temporal local Gi\* values,

-   Select three sub-districts and describe the temporal trends revealed (not more than 250 words), and

-   Prepared a EHSA map of the Gi\* values of vaccination rate.
    The maps should only display the significant (i.e. p-value \< 0.05).

-   With reference to the EHSA map prepared, describe the spatial patterns revelaed.
    (not more than 250 words).

Throughout this page, each step will be explained and guided so that you can follow along.

# The Set Up

## Packages Used

The R packages used for this analysis are:

-   sf

-   tidyverse

-   spatstat

-   tmap

-   sfdep

-   maptools

-   readxl

The code chunk below checks whether the packages have been installed, if not it will automatically install them and load the packages into Rstudio.

```{r}
pacman::p_load(sf, raster, spatstat, tmap, tidyverse, sfdep, maptools, readxl, plotly)
```

## The Data

+----------------+------------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------+
| Type           | Name                                                                                                                   | Format         | Description                                                                   |
+================+========================================================================================================================+================+===============================================================================+
| Geospatial     | [Batas Desa Provinsi DKI Jakarta](https://www.indonesia-geospasial.com/2020/04/download-shapefile-shp-batas-desa.html) | shapefile      | Sub-districts in DKI Jakarta                                                  |
+----------------+------------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------+
| Aspatial       | [Riwayat File Vaksinasi DKI Jakarta](https://riwayat-file-vaksinasi-dki-jakarta-jakartagis.hub.arcgis.com/)            | .xlsx          | Sub-district level data of vaccination numbers between July 2021 to June 2022 |
+----------------+------------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------+

-   Geospatial Data

The link under Geospatial Data above brings you to a page where there are many download links sorted by province.
Ensure that you are using **Shapefile (SHP) Batas Desa Provinsi DKI Jakarta.**

-   Aspatial Data

The link under Aspatial Data above brings you to a page where there are two types of data files you can use.
Please choose **Data Vaksinasi Berbasis Kelurahan dan Kecamatan** and download a total of 12 files beginning July 2021 to June 2022.

Do note that we will be using the beginning of each month for our download.

# Data Wrangling

## Geospatial Data

### Import Geospatial Data

```{r}
geoDKI <- st_read(dsn = "data/geospatial",
                    layer = "BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA")
```

From the output above, we can see that the data set has a geometry type, Multipolygon, and has 269 features and 161 fields.

### Check for invalid geometries

Before we begin, we should check whether there are any invalid geometries by using the code chunk below:

```{r}
st_is_valid(geoDKI)
```

st_is_valid() from the sf package helps to check whether a geometry is valid.
From the output, there are no invalid geometries.

### Check for Missing values

The code chunk below uses is.na() from base R checks whether the data set has NA values.
which() from base R takes the indices of these values and lastly length() helps us calculate the length of the data objects.

```{r}
length(which(is.na(geoDKI) == TRUE))
```

In the above output, there are 14 NA values within the jakarta data set.
We will carry on with these 14 NA values and will be further explained later.

### Check Coordinate System

As different countries use different projection systems, we need to first check the CRS of jakarta.

The code chunk below uses st_crs() from the sf package:

```{r}
st_crs(geoDKI)
```

We notice that jakarta is using EPSG::4326 which is the wrong projection coordinate system.
DKI Jakarta uses the DGN95, the 'Datum Geodesi Nasional 1995', EPSG::23878

We can transform the crs by using st_transform() from the sf package:

```{r}
geoDKI <- geoDKI %>%
  st_transform(crs = 23878)
```

```{r}
st_crs(geoDKI)
```

From the output above, we can see that the CRS has been properly assigned.

### Removing Outer Islands

Let us visualise the geographical polygons.

```{r}
qtm(geoDKI)
```

We can see from the output that jakarta includes both the mainland and the outer islands.
Our study area focuses only on the mainland and thus we need to remove them.

```{r, eval = FALSE}
View(geoDKI)
```

Before we continue further, we need to understand how DKI Jakarta geographical regions are segmented.
The code chunk above lets you view the entire dataset.
Let us understand the key variables below:

With the help of uncle google, we will translate the names.

| Name      | Translation                       |
|-----------|-----------------------------------|
| KODE_DESA | Village Code (Sub-District Codes) |
| DESA      | Village                           |
| PROVINSI  | Province                          |
| KAB_KOTA  | City                              |
| KECAMATAN | Sub-District                      |

KAB_KOTA would be the most logical choice in isolating out the outer islands.
This will be shown in the plots later.

The code chunk below helps to output unique values in KAB_KOTA field.

```{r}
unique(geoDKI$KAB_KOTA)
```

The output above shows that there are 6 major cities in DKI Jakarta, ignoring the NA value.

The codechunk below will visualize the data with respect to the 6 major cities.
We can then see which city isolates out the outer islands.

```{r}
tmap_mode("view")
tm_shape(geoDKI) +
  tm_polygons("KAB_KOTA") +
  tm_view(set.zoom.limits = c(9,14))
```

#### Dealing with NA values

Before we filter out the outer islands, we have to fix the NA values within geoDKI.

The NA values in geoDKI pertains to the information of small areas that is shaded white, shown in the map above between JAKARTA UTARA and JAKARTA PUSAT

By viewing geoDKI, row 243 and 244 is where all the 14 NA reside at, as mentioned above.
From checking the area bounds online, the white sub-districts pertains to city region, JAKARTA UTARA. As we could not find the KODE DESA (village code), we will just assign unique values: 3188888801 and 3188888802 for now.

This ensures completeness in our geographical plots later on.

```{r}
geoDKI$KAB_KOTA[243]<-"JAKARTA UTARA"
geoDKI$KAB_KOTA[244]<-"JAKARTA UTARA"

geoDKI$KODE_DESA[243]<-"3188888801"
geoDKI$KODE_DESA[244]<-"3188888802"
```

#### Filtering out Outer Islands

From the visualization above, we can see that KEPULAUAN SERIBU is not part of the mainland.
We can then use filter() from dplyr package to remove the outer islands.

```{r}
geoDKI <- filter(geoDKI, KAB_KOTA !="KEPULAUAN SERIBU")
```

```{r}
tmap_mode("plot")
tm_shape(geoDKI) + 
  tm_polygons("KAB_KOTA")
```

On top of getting the mainland study area, we notice the two white shaded areas are now under JAKARTA UTARA (Blue).

### Filtering Relevant Columns

Now, before we move on further, geoDKI has alot of columns.
Since we are only interested in the subdistrict level, we will select KODE_DESA and rename it to village_code.

Note: village_code is also known as the subdistrict code.

```{r}
geoDKI <- geoDKI %>%
  select(2, "geometry") %>%
  rename(village_code = `KODE_DESA`)
```

## Aspatial Data

### Import Aspatial Data

Before we import the data in, the file names are rather long.
First, go to your data folder and change the aspatial data file names to Y-M format.
It would look like this:

![](images/image-1137647241.png){fig-align="center" width="488"}

There are a total of 12 excel files we need to load into Rstudio.
To read the files more efficiently, we will use the "for loop" to read all the excel files into a data frame by using the read_excel() from the readxl package.

```{r}
# Set the working directory to the folder containing the Excel files
setwd("data/aspatial/")
# Get a list of all Excel files in the directory
aspatial_data <- list.files(pattern = ".xlsx")

# Loop through the files and read each one into a data frame
for (i in aspatial_data) {
  assign(gsub(".xlsx", "", i), read_excel(i))
}
```

### Columns of interest and its translation

In the aspatial data set, what we want is the total vaccination and not vaccinated numbers, along with these 4 regional classification columns.

```{r}
names(`2022-6`)
```

| Name           | Translation    |
|----------------|----------------|
| KODE KELURAHAN | Village Code   |
| WILAYAH KOTA   | City Region    |
| SASARAN        | Vaccinated     |
| BELUM VAKSIN   | Not Vaccinated |

### Mutate Aspatial Data

#### Mutating using for loop

As there are 12 data sets, we will use the for loop that mutate, rename and select the fields that we want.
The code chunk below does the following:

1.  Renames KODE KELURAHAN, WILAYAH KOTA, SASARAN and BELUM VAKSIN to village_code, city_region, vaccinated and not_vaccinated respectively by using rename() from the dplyr package
2.  Selects the renamed columns by using select() from the dplyr package
3.  Adds a date column by using mutate() from the dplyr package

All the mutated aspatial data frames are then placed into a list

```{r}
list_mth<- list(`2021-7`,`2021-8`,`2021-9`,`2021-10`,`2021-11`,`2021-12`,`2022-1`,`2022-2`,`2022-3`,`2022-4`,`2022-5`,`2022-6`)

date <- c("2021-07-01", "2021-08-01", "2021-09-01", "2021-10-01", "2021-11-01", "2021-12-01",
          "2022-01-01", "2022-02-01", "2022-03-01","2022-04-01", "2022-05-01", "2022-06-01")

lists <- list()
for (i in c(1:12)){
  lists[[i]]<- list_mth[[i]] %>% 
    rename(village_code=`KODE KELURAHAN`,
           city_region =`WILAYAH KOTA`,
           target= `SASARAN`, 
           not_vaccinated =`BELUM VAKSIN`) %>% 
  select(village_code,city_region, not_vaccinated ,target) %>%
    mutate(date = as.Date(date[i]), 
           .before=1)
}
```

#### Combine into a single dataframe from a list of dataframes

Afterwhich, we can use Reduce() and rbind from base R to join all dataframes in the lists as one dataframe.

The code chunk below does this:

```{r}
aspatial_data <- Reduce(rbind, lists)
glimpse(aspatial_data)
```

We can see from the output that we have the columns that we want in its new name and having 3216 rows.

From the glimpse output, we can see that there are NA values inside the dataframe.
If you View the original individual files, you will notice that there will be a row that calculates the total of a respective column and that row contains NA values.

#### Final steps to take

So these are the steps to take,

1.  Remove the NA rows in the dataframe
2.  Filter out the outer islands
    -   You will notice from the output of the code chunk below that outer islands is categorised as KAB.ADM.KEP.SERIBU

```{r}
unique(aspatial_data$city_region)
```

3.  Add in vaccination rate column
    -   Formula:
        -   vaccination_rate = (target - not_vaccinated)/target

The code chunk below does this:

```{r}
aspatial_data <- aspatial_data %>%
  na.exclude() %>%
  filter(city_region != "KAB.ADM.KEP.SERIBU") %>% 
  mutate(vaccination_rate = as.numeric((target-not_vaccinated)/target)) %>%
  select(date, village_code, vaccination_rate)
```

4.  Add in village code: 3188888801 and 3188888802

Recall that there are two polygons that held missing values.

```{r}
setdiff(geoDKI$village_code, aspatial_data$village_code)
```

We notice that in the aspatial data, no information were collected for these two sub-districts.
Hence, we have to add in these two subdistricts so that both dataframes will match when it is joined later on.

Note: Each month must have one observation, vaccination rate for the added rows will be considered NA, missing data.

```{r}
aspatial_data <- rbind(aspatial_data, c("2021-07-01", 3188888801,NA),
                       c("2021-08-01", 3188888801,NA),
                       c("2021-09-01", 3188888801,NA),
                       c("2021-10-01", 3188888801,NA),
                       c("2021-11-01", 3188888801,NA),
                       c("2021-12-01", 3188888801,NA),
                       c("2022-01-01", 3188888801,NA),
                       c("2022-02-01", 3188888801,NA),
                       c("2022-03-01", 3188888801,NA),
                       c("2022-04-01", 3188888801,NA),
                       c("2022-05-01", 3188888801,NA),
                       c("2022-06-01", 3188888801,NA),
                       c("2021-07-01", 3188888802,NA),
                       c("2021-08-01", 3188888802,NA),
                       c("2021-09-01", 3188888802,NA),
                       c("2021-10-01", 3188888802,NA),
                       c("2021-11-01", 3188888802,NA),
                       c("2021-12-01", 3188888802,NA),
                       c("2022-01-01", 3188888802,NA),
                       c("2022-02-01", 3188888802,NA),
                       c("2022-03-01", 3188888802,NA),
                       c("2022-04-01", 3188888802,NA),
                       c("2022-05-01", 3188888802,NA),
                       c("2022-06-01", 3188888802,NA))
```

Both aspatial_data and geoDKI have the same number of unique village code.\

The code chunk below joins them together by their village code, sets vaccination_rate as numeric in the joined dataframe.

```{r}
vaccination <- left_join(aspatial_data, geoDKI, 
                         by = c("village_code")) %>%
  mutate(vaccination_rate = as.numeric(vaccination_rate))
```

We can save the vaccination sf dataframe as a file so that we can call upon it without having to run the above codes.

Furthermore, when we transfer this file to other people, they will notice the two polygons with missing data more clearly when analysing the datafile.

```{r, echo = FALSE}
saveRDS(vaccination, "vaccination.rds")
```

Afterwhich, we can then exclude the NA values and convert the object to sf.

```{r}
vaccination <- vaccination %>%
  na.exclude() %>%
  st_as_sf()
```

# Choropleth Mapping and Analysis

## Visualizing the data

### R Shiny

Something extra to this TakeHome Assignment will be the use of the ShinyApp.
This is a short introduction as to how Rshiny works:

1.  Define the UI:
    -   You will have an input and in this case, we will use selectInput("dates", "Pick a month", date, selected = "July 2021",multiple = FALSE).

        -   "dates": This is the variable name to call into the output portion in the server

        -   "Pick a month": This is a text under the user interface to ask the user to pick a choice

        -   date: This is the vector of choices that they can pick from.

        -   selected = "July 2021" : This sets the choice "July 2021" as the default when starting up the app

        -   multiple = FALSE : Prevents the user from picking multiple options and can only choose 1
2.  Define Server:
    -   This is where tmap is used and calls upon the variable name defined in UI which is "dates".

```{r, eval = FALSE}
library(shiny)

date <- c("2021-07-01", "2021-08-01", "2021-09-01", "2021-10-01", "2021-11-01", "2021-12-01", "2022-01-01", "2022-02-01", "2022-03-01", "2022-04-01", "2022-05-01", "2022-06-01")
ui <- fluidPage(
  selectInput("dates", "Pick a date",
              date, selected = "2021-07-01",
              multiple = FALSE),

  tmapOutput("my_map")
)

# Define the server
server <- function(input, output) {
  # Render the tmap in the output element
  output$my_map <- renderTmap({
    a <- vaccination |>
      filter(date == input$dates)
    
    tm_shape(a) +
      tm_fill("vaccination_rate",
              n = 6,
              style = "quantile",
              palette = "Blues",
              title = "Vaccination Rate") +
      tm_layout(main.title = paste(input$dates),
                main.title.position = "left",
                legend.height = 0.8, 
                legend.width = 0.8,
                frame = TRUE) +
      tm_borders(alpha = 0.5) +
      tm_grid(alpha =0.2)
  })
}

# Run the app
shinyApp(ui, server)

```

Refer to visual plot : [ShinyApp](https://junhaoteo.shinyapps.io/Take-Home_Ex02/)

Note: There are many ways to create and design the shiny app, under the user interface.
This can be found under R shiny documentation.

### Tmap Visualization

Considering we have 12 months of Tmap visualization to do, it would be better to create a tmap function.

The code chunk below first filters out vaccination dataframe into their respective months and then inputs the filtered dataframe into the tm_shape().

```{r}
graphing <- function(x){
  a <- vaccination %>%
    filter(date == x)
  tm_shape(a) +
  tm_fill("vaccination_rate",
          n=10,
          style = "quantile",
          palette = "Blues",
          title = "Vaccination Rate") +
  tm_layout(main.title = paste(x),
            main.title.position = "left",
            legend.height = 0.8, 
            legend.width = 0.8,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_grid(alpha =0.2)
}
```

We will split the plots into 2 code chunks to reduce the number of graphs in a single output for a clearer view.

```{r}
tmap_mode("plot")
tmap_arrange(graphing("2021-07-01"),
             graphing("2021-08-01"),
             graphing("2021-09-01"),
             graphing("2021-10-01"),
             graphing("2021-11-01"),
             graphing("2021-12-01"),
             ncol = 2)
```

```{r}
tmap_arrange(graphing("2022-01-01"),
             graphing("2022-02-01"),
             graphing("2022-03-01"),
             graphing("2022-04-01"),
             graphing("2022-05-01"),
             graphing("2022-06-01"),
             ncol = 2)
```

Likewise, this can be done on Rshiny app but this will be covered on a later date when I have attended the Rshiny workshop.
:)

## Spatial Patterns observed (200words)

Firstly, from the choropleth maps seen above, we notice that the southern part of DKI Jakarta started out with lower vaccination rates but in 2022-06, it became the region with one of the highest vaccination rates.
Another observation is that the northern region of DKI Jakarta gradually over the months had vaccination rates lower than the other parts.
However, this does not mean that vaccination rates are decreasing in the northern region.
One of the distinct change is through the classification numbers.
In 2021-07, we notice that the minimum vaccination rate is 0.227 and that number increases as the months go by to 0.781 in 2022-06.
Overall, we can conclude that vaccination rates throughout DKI Jakarta have seen an increase and a more stable distribution across its sub-district.

# Local Gi\* Analysis

To bring you back, here is the task assigned in this section:

-   Compute local Gi\* values of the monthly vaccination rate,

-   Display the Gi\* maps of the monthly vaccination rate.
    The maps should only display the significant (i.e. p-value \< 0.05)

-   With reference to the analysis results, draw statistical conclusions (not more than 250 words)

## Computing Contiguity Spatial Weights

Before we can compute the global spatial autocorrelation statistics, we need to construct the spatial weights of the study area.

Spatial weights: Used to define the neighbourhood relationships between geographical units in the study area.

As we are dealing with 12 different months of vaccination rates, we will have to filter the dataframe to their respective months and calculate their respective weights.
The code chunk below shows the function that filters to the selected month and then calculate their weights.

```{r}
month <- vaccination %>%
  filter(date == "2021-07-01")
wm_q <- month %>%
  na.exclude() %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_inverse_distance(nb, geometry,
                                   scale = 1,
                                   alpha = 1),
         .before = 1)
wm_q
```

## Computing local GI

Next, we will use local_gstar_perm() of the sfdep package to calculate local GI.

We will also need to use set.seed() to ensure we obtain the same result when the code chunk is re-runned.

```{r}
set.seed(1234)
july_LGI <- wm_q %>%
  mutate(local_Gi = local_gstar_perm(vaccination_rate,
                                   nb,
                                   wt,
                                   nsim = 99),
         .before = 1) %>%
  unnest(local_Gi)
```

## Visualizing p-value of local Gi

```{r}
tmap_mode("plot")
tm_shape(july_LGI) +
    tm_polygons() +
    tm_shape(july_LGI %>% filter(p_sim <0.05)) +
    tm_fill("gi_star") +
    tm_borders(alpha = 0.5) +
    tm_layout(main.title = paste("significant local Gi", "(", july_LGI$date[1],")"),
              main.title.size = 0.8)
```

## Putting it all together

As you follow along, these are the 3 main steps to creating the plots required for this section.
As mentioned above, we will combine these 3 sections into a single function to calculate the local Gi for each month.
The code chunk below takes in input "mth" to filter vaccination to the respective month and calculate the weights and the local Moran's I.

### Creating the local GI computation function

```{r}
local_gi <- function(mth){
  set.seed(1234)
  month <- vaccination %>%
    filter(date ==mth)
  wm_q <- month %>%
    na.exclude() %>%
    mutate(nb = st_contiguity(geometry),
           wt = st_inverse_distance(nb, geometry,
                                   scale = 1,
                                   alpha = 1))
  result <- wm_q %>%
    mutate(local_gi = local_gstar_perm(vaccination_rate,
                                   nb,
                                   wt,
                                   nsim = 99),
           .before = 1) %>% 
    unnest(local_gi)
  return(result)
}
```

Now we will calculate the local Gi for each month and place it into a list for easy referencing for the plots later.

```{r}
date <- c("2021-07-01", "2021-08-01", "2021-09-01", "2021-10-01", "2021-11-01", "2021-12-01",
          "2022-01-01", "2022-02-01", "2022-03-01","2022-04-01", "2022-05-01", "2022-06-01")
month_GI <- list()
for (i in 1:12){
  month_GI[[i]] <- local_gi(date[i])
}
```

Now we have a list of dataframes with local GI computed.
Note that lisa_LMI\[\[1\]\] refers to July 2021, in ascending order.

### Creating the Tmap function

```{r}
graph_gi <- function(x){
  HCSA_sig <- x %>%
    filter(p_sim <0.05)
  HCSA_plots <- tm_shape(x) +
    tm_polygons() +
    tm_borders(alpha = 0.5) +
    tm_shape(HCSA_sig) +
      tm_fill("gi_star",
              palette = "-RdBu",
              midpoint = 0) +
      tm_borders(alpha = 0.4) +
    tm_layout(main.title = paste("significant local Gi", "(",x$date[1],")"),
              main.title.size = 0.8)
  return(HCSA_plots)
}
```

```{r}
tmap_mode("plot")
tmap_arrange(graph_gi(month_GI[[1]]),
             graph_gi(month_GI[[2]]),
             graph_gi(month_GI[[3]]),
             graph_gi(month_GI[[4]]),
             graph_gi(month_GI[[5]]),
             graph_gi(month_GI[[6]]),
             ncol =2)
```

```{r}
tmap_arrange(graph_gi(month_GI[[7]]),
             graph_gi(month_GI[[8]]),
             graph_gi(month_GI[[9]]),
             graph_gi(month_GI[[10]]),
             graph_gi(month_GI[[11]]),
             graph_gi(month_GI[[12]]),
             ncol =2)
```

Refer to this [link](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/h-how-hot-spot-analysis-getis-ord-gi-spatial-stati.htm) on Hotspot and Coldspot

## Statistical Conclusion (Not more than 250words) :

The graph plotted above showcases the hot spots and cold spots.
Hot spots are referred to the colour red with a positive z-value while cold spots are referred to the colour blue with a negative z-value.
Since the plots were filtered by p-value being less than 0.05, they are statistically significant at the 0.05 level of significance.

In the earlier months, we notice that hot spots are generally in the northern region of DKI Jakarta while cold spots are generally in the middle.
We start to see a shift in hotspots moving towards the southern region in the later months.
This evidence is also seen in the spatial patterns observed in the choropleth maps, where higher vaccination rates shifts from north to south.
It is also worth noting that the cold spots at the center have been consistent across the months and this could indicate either lack of willingness to be vaccinated in the sub-district or a lack of medical capacity.
However, as mentioned above, overall vaccination rates have increased over the months.
If the goal is to increase vaccination rates across DKI Jakarta, the local Gi maps can indicate areas of concern (cold spots) to place greater priority.

# Emerging Hot Spot Analysis (EHSA)

## Creating a Time Series Cube

In creating a Time series cube, there are two arguments that needs to be satisfied, a location identifier and a time identifier.
As taught in in-class Exercise 7, we can use spacetime() from the sfdep package to create the time series cube.
This is done by taking the .data and .geometry and combining them together.

The code chunk below is done as taught in class:

```{r, eval = FALSE}
vaccination_rate_st <- spacetime(aspatial_data, 
                                 geoDKI,
                                 .loc_col = "village_code",
                                 .time_col = "date")
```

Another way is to use as_spacetime() from the sfdep package.
It takes the combined dataframe.

The code chunk below creates a spatio-temporal cube by using spacetime() from the sfdep package.

```{r}
vaccination_rate_st<- as_spacetime(vaccination,
                                .loc_col = "village_code",
                                .time_col = "date")
```

The code chunk below checks whether the object is a spacetime cube by using is_spacetime_cube from the sfdep package.

```{r}
is_spacetime_cube(vaccination_rate_st)
```

## Deriving Spatial Weights

```{r}
vaccination_rate_nb <- vaccination_rate_st %>%
  activate("geometry") %>%
  mutate(
    nb = include_self(st_contiguity(geometry)),
    wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
    .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

## Computing Gi\*

The codechunk below uses local_gstar_perm from the sfdep package to compute the Gi\*:

```{r}
gi_stars <- vaccination_rate_nb %>%
  group_by(date) %>%
  mutate(vaccination_rate = as.numeric(vaccination_rate),
         gi_star = local_gstar_perm(vaccination_rate, 
                                    nb, 
                                    wt, 
                                    nsim = 99)) %>%
  tidyr::unnest(gi_star)
```

## Mann-Kendall Test

The Mann-Kendall test is a statistical test used in spatial analysis to detect trends and changes.
However, it does not detect magnitude of changes.

The assignment requires us to select 3 sub-districts and describe the temperal trends revealed.

Sub-districts Chosen:

1.  3174101005
2.  3173031006
3.  3174091001

### District 1: 3174101005

We will first visualize the distribution of Gi\* value to the corresponding village_code

```{r}
cbg_1 <- gi_stars %>%
  ungroup() %>%
  filter(village_code == "3174101005") %>%
  select(date, village_code, gi_star)
```

```{r}
ggplot(data = cbg_1, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```

We can then use MannKendall() from the Kendall package to calculate the test statistics of the Mann Kendall test.

The output we will be focusing on are:

1.  tau : A measure of the strength and direction of the trend in the data. It ranges from -1 to 1, where a tau value of -1 indicates a strong negative trend, a tau value of 0 indicates no trend, and a tau value of 1 indicates a strong positive trend.
2.  sl : This is the p value and for it to be significant, it has to be less than significance level, 0.05.

```{r}
test_stat1 <- cbg_1 %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
glimpse(test_stat1)
```

#### Statistical conclusion:

For the 1st sub-district chosen, we notice from the plot that gi\* values is in the negative suggesting that it is a cold spot.
It is also seen that gi\* values are getting larger as months go by, indicating that vaccination rates are improving.
This is supported by the Mann Kendell test where there is a positive association between gi\* values and date as tau = 0.575 \> 0.
It is also statistically significant at the 0.05 level of significance since sl \< 0.05.

### District 2: 3173031006

Repeat the steps above

```{r}
cbg_2 <- gi_stars %>%
  ungroup() %>%
  filter(village_code == "3173031006") %>%
  select(date, village_code, gi_star)
```

```{r}
ggplot(data = cbg_2, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()

```

```{r}
test_stat2 <- cbg_2 %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
glimpse(test_stat2)
```

#### Statistical Conclusion 2:

From the plot above, we notice that gi\* values increase in the initial month and then fluctuate but remain relatively consistent.
However while tau \> 0, it is close to 0 and that there is no trend since sl = 0.837 \> 0.05.
Hence the result is statistically insignificant and we can conclude there is no trend between gi\* values and date for District 2.
This suggest that for district 2, the gi\* values have no trend and base on the plot, there is clustering but it remains relatively consistent.

### District 3: 3174091001

Repeat steps above

```{r}
cbg_3 <- gi_stars %>%
  ungroup() %>%
  filter(village_code == "3174091001") %>%
  select(date, village_code, gi_star)
```

```{r}
ggplot(data = cbg_3, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
```

```{r}
test_stat3 <- cbg_3 %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
glimpse(test_stat3)
```

#### Statistical Conclusion 3:

Similar to the first district, there is a positive association between gi\* values and date as tau =0.545 \> 0.
According to the plot, we also see a strong positive associate where gi\* values increases as the month goes by.
This result is also statistically significant since sl = 0.016 \< 0.05.
This suggest that there is greater clustering of hotspots, suggesting higher vaccination rates as gi\* values increases.

## Performing Emerging Hotspot Analysis

Firstly, we will run the gi\* values from all the vilage code using the same steps above.
By using the MannKendall(), we can calculate their relevant test statistics.
Afterwhich, we will filter by their sl values to those significant.

```{r}
ehsa <- gi_stars %>%
  group_by(date) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk) %>%
  filter(sl < 0.05)
```

### Arrange to show significant emerging hotspots and coldspots

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)
```

The code chunk below uses the emerging_hotspot_analysis() from the sf package.

Arguments:

1.  x takes in a spacetime object
2.  .var takes in the variable of interest, in this case is vaccination_rate
3.  k refers to the number of time lags.

```{r}
set.seed(1234)
ehsa <- emerging_hotspot_analysis(
  x = vaccination_rate_st,
  .var = "vaccination_rate",
  k = 1,
  nsim = 99
)
```

### Visualising distribution of EHSA

We can visualise the data using ggplot to see the distribution of the classifications.

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

### Visualising EHSA

We will first need to join the ehsa dataframe with our classification column.
This will be joined together by the unique identifier "village_code" and "location".

```{r}
vaccination_ehsa <- left_join(geoDKI, ehsa, by = c("village_code" = "location"))
```

```{r}
ehsa_sig <- vaccination_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("view")
tm_shape(vaccination_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```

## Statistical Conclusion (Not more than 250 words)

We can refer to the table below obtained from this [link](https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/learnmoreemerging.htm#:~:text=Oscillating%20Cold%20Spot,been%20statistically%20significant%20cold%20spots)

+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| \                         | Definition                                                                                                                                                                                                                                                                                                                 |
| Pattern name              |                                                                                                                                                                                                                                                                                                                            |
+===========================+============================================================================================================================================================================================================================================================================================================================+
| **No Pattern Detected**   | Does not fall into any of the hot or cold spot patterns defined below.                                                                                                                                                                                                                                                     |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| New Hot Spot              | A location that is a statistically significant hot spot for the final time step and has never been a statistically significant hot spot before.                                                                                                                                                                            |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Consecutive Hot Spot      | A location with a single uninterrupted run of at least two statistically significant hot spot bins in the final time-step intervals. The location has never been a statistically significant hot spot prior to the final hot spot run and less than 90 percent of all bins are statistically significant hot spots.        |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Intensifying Hot Spot     | A location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of high counts in each time step is increasing overall and that increase is statistically significant.                                    |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Persistent Hot Spot       | A location that has been a statistically significant hot spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering over time.                                                                                                                                                |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Diminishing Hot Spot      | A location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering in each time step is decreasing overall and that decrease is statistically significant.                                                   |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Sporadic Hot Spot**     | A statistically significant hot spot for the final time-step interval with a history of also being an on-again and off-again hot spot. Less than 90 percent of the time-step intervals have been statistically significant hot spots and none of the time-step intervals have been statistically significant cold spots.   |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Oscillating Hot Spot**  | A statistically significant hot spot for the final time-step interval that has a history of also being a statistically significant cold spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant hot spots.                                                      |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Historical Hot Spot       | The most recent time period is not hot, but at least 90 percent of the time-step intervals have been statistically significant hot spots.                                                                                                                                                                                  |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| New Cold Spot             | A location that is a statistically significant cold spot for the final time step and has never been a statistically significant cold spot before.                                                                                                                                                                          |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Consecutive Cold Spot     | A location with a single uninterrupted run of at least two statistically significant cold spot bins in the final time-step intervals. The location has never been a statistically significant cold spot prior to the final cold spot run and less than 90 percent of all bins are statistically significant cold spots.    |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Intensifying Cold Spot    | A location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is increasing overall and that increase is statistically significant.                                    |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Persistent Cold Spot      | A location that has been a statistically significant cold spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering of counts over time.                                                                                                                                     |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Diminishing Cold Spot     | A location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is decreasing overall and that decrease is statistically significant.                                    |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Sporadic Cold Spot**    | A statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots. |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Oscillating Cold Spot** | A statistically significant cold spot for the final time-step interval that has a history of also being a statistically significant hot spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant cold spots.                                                     |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Historical Cold Spot      | The most recent time period is not cold, but at least 90 percent of the time-step intervals have been statistically significant cold spots.                                                                                                                                                                                |
+---------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

By referring to the definitions above, an oscillating hot spot implies that many sub-districts that were cold-spots initially became hotspots, indicating that there is a shift in clustering of low to high vaccination rates.
From the distribution of classification, we know that there is more oscillating hotspots than oscillating cold spots and thus this tells us that more sub-districts had higher vaccination rates over time.
This is as expected and ideal as DKI Jakarta is undergoing the inoculation program initiated by the government.

An sporadic cold spot, according to the definition above, implies that these regions generally switch between statistically significant and statistically insignificant cold spots.
These areas could be investigated further to understand why vaccination rates has not improved as quickly as the other regions.

With that, we are done!
